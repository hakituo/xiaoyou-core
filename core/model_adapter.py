import os
import logging
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForVision2Seq
from diffusers import StableDiffusionPipeline
from PIL import Image
import gc

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelAdapter:
    """
    å®Œæ•´åŠŸèƒ½æ¨¡å‹é€‚é…å™¨ï¼Œæ”¯æŒæ–‡æœ¬ç”Ÿæˆã€è§†è§‰æ¨¡å‹å’Œå›¾åƒç”ŸæˆåŠŸèƒ½ã€‚
    ä¼˜åŒ–æ¨¡å‹åŠ è½½å’Œå†…å­˜ç®¡ç†ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£è®¿é—®ä¸åŒç±»å‹çš„æ¨¡å‹ã€‚
    """
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–æ¨¡å‹é€‚é…å™¨
        
        Args:
            config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«å„ç§æ¨¡å‹è·¯å¾„å’Œå‚æ•°
        """
        # é»˜è®¤é…ç½®ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡å‹ç±»å‹
        self.config = config or {
            "device": "auto",  # 'cuda', 'cpu', or 'auto'
            "text_model_path": "./models/qwen",
            "vision_model_path": "./models/qwen",
            "image_gen_model_path": "./models/sd",
            "low_cpu_mem_usage": True,
            "max_new_tokens": 512,
            "temperature": 0.7
        }
        
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if self.config["device"] == "auto":
            self.config["device"] = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {self.config['device']}")
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å¼•ç”¨
        self.text_model = None
        self.text_tokenizer = None
        self.vision_model = None
        self.vision_tokenizer = None
        self.image_gen_model = None
        
        # æ ‡è®°å„æ¨¡å‹åŠ è½½çŠ¶æ€
        self.model_loaded = {
            "text": False,
            "vision": False,
            "image_gen": False
        }
    
    def _clear_memory(self):
        """
        æ¸…ç†å†…å­˜å’Œæ˜¾å­˜ï¼Œä¼˜åŒ–å¤šæ¨¡å‹åˆ‡æ¢
        """
        try:
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            logger.info("å†…å­˜æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†å†…å­˜æ—¶å‡ºé”™: {str(e)}")
    
    def _load_text_model(self):
        """
        åŠ è½½æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œåˆ†è¯å™¨
        """
        try:
            model_path = self.config["text_model_path"]
            logger.info(f"æ­£åœ¨åŠ è½½æ–‡æœ¬æ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œè¯·ç¡®ä¿æœ¬åœ°å·²ä¸‹è½½æ¨¡å‹")
                return None, None
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "low_cpu_mem_usage": self.config["low_cpu_mem_usage"],
                "torch_dtype": torch.float16 if self.config["device"] == "cuda" else torch.float32,
                # ç¦ç”¨ä»Hugging Faceä¸‹è½½
                "local_files_only": True
            }
            
            # æ·»åŠ è®¾å¤‡æ˜ å°„
            if self.config["device"] == "cuda":
                model_kwargs["device_map"] = "auto"
            
            # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
            
            # å¦‚æœæ²¡æœ‰ä½¿ç”¨device_mapï¼Œåˆ™æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
            if self.config["device"] != "cuda" or not model_kwargs.get("device_map"):
                model = model.to(self.config["device"])
            
            logger.info("æ–‡æœ¬æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æœ¬æ¨¡å‹å¤±è´¥: {str(e)}")
            return None, None
    
    def _load_vision_model(self):
        """
        åŠ è½½è§†è§‰æ¨¡å‹å’Œåˆ†è¯å™¨
        """
        try:
            model_path = self.config["vision_model_path"]
            logger.info(f"æ­£åœ¨åŠ è½½è§†è§‰æ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œè¯·ç¡®ä¿æœ¬åœ°å·²ä¸‹è½½æ¨¡å‹")
                return None, None
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "low_cpu_mem_usage": self.config["low_cpu_mem_usage"],
                "torch_dtype": torch.float16 if self.config["device"] == "cuda" else torch.float32,
                # ç¦ç”¨ä»Hugging Faceä¸‹è½½
                "local_files_only": True
            }
            
            # æ·»åŠ è®¾å¤‡æ˜ å°„
            if self.config["device"] == "cuda":
                model_kwargs["device_map"] = "auto"
            
            # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            model = AutoModelForVision2Seq.from_pretrained(model_path, **model_kwargs)
            
            # å¦‚æœæ²¡æœ‰ä½¿ç”¨device_mapï¼Œåˆ™æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
            if self.config["device"] != "cuda" or not model_kwargs.get("device_map"):
                model = model.to(self.config["device"])
            
            logger.info("è§†è§‰æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"åŠ è½½è§†è§‰æ¨¡å‹å¤±è´¥: {str(e)}")
            return None, None
    
    def _load_image_gen_model(self):
        """
        åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹
        """
        try:
            model_path = self.config["image_gen_model_path"]
            logger.info(f"æ­£åœ¨åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œè¯·ç¡®ä¿æœ¬åœ°å·²ä¸‹è½½æ¨¡å‹")
                return None
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            pipe_kwargs = {
                # ç¦ç”¨ä»Hugging Faceä¸‹è½½
                "local_files_only": True
            }
            
            # æ·»åŠ è®¾å¤‡
            if self.config["device"] == "cuda":
                pipe_kwargs["torch_dtype"] = torch.float16
                pipe_kwargs["device_map"] = "auto"
            
            # åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹
            pipe = StableDiffusionPipeline.from_pretrained(model_path, **pipe_kwargs)
            
            # å¦‚æœéœ€è¦ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°CUDA
            if self.config["device"] == "cuda" and not pipe_kwargs.get("device_map"):
                pipe = pipe.to("cuda")
            
            logger.info("å›¾åƒç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            return pipe
            
        except Exception as e:
            logger.error(f"åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹å¤±è´¥: {str(e)}")
            return None
    
    def chat(self, prompt, max_tokens=None, temperature=None):
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            max_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            åŒ…å«çŠ¶æ€å’Œå“åº”çš„å­—å…¸
        """
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å‚æ•°
            max_tokens = max_tokens or self.config.get("max_new_tokens", 512)
            temperature = temperature or self.config.get("temperature", 0.7)
            
            # å»¶è¿ŸåŠ è½½æ¨¡å‹
            if self.text_model is None and not self.model_loaded["text"]:
                # éœ€è¦æ¸…ç†å†…å­˜ä»¥åŠ è½½æ–°æ¨¡å‹
                if self.vision_model or self.image_gen_model:
                    self._clear_memory()
                self.text_model, self.text_tokenizer = self._load_text_model()
                self.model_loaded["text"] = True
            
            if not self.text_model or not self.text_tokenizer:
                return {
                    "status": "error",
                    "error": "æ–‡æœ¬æ¨¡å‹åŠ è½½å¤±è´¥æˆ–ä¸å¯ç”¨"
                }
            
            # åˆ†è¯è¾“å…¥
            inputs = self.text_tokenizer(prompt, return_tensors="pt")
            
            # å°†è¾“å…¥ç§»è‡³ç›¸åº”è®¾å¤‡
            if self.config["device"] == "cuda":
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                output = self.text_model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.text_tokenizer.eos_token_id,
                    bos_token_id=self.text_tokenizer.bos_token_id,
                    eos_token_id=self.text_tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”ï¼Œè·³è¿‡è¾“å…¥éƒ¨åˆ†
            response = self.text_tokenizer.decode(
                output[0][len(inputs["input_ids"][0]):],
                skip_special_tokens=True
            )
            
            return {
                "status": "success",
                "response": response
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            return {
                "status": "error",
                "error": f"ç”Ÿæˆé”™è¯¯: {str(e)}"
            }
    
    def describe_image(self, image, prompt=None, max_tokens=512):
        """
        ä½¿ç”¨è§†è§‰æ¨¡å‹æè¿°å›¾åƒ
        
        Args:
            image: PIL Imageå¯¹è±¡æˆ–å›¾åƒè·¯å¾„
            prompt: å¯é€‰çš„æç¤ºæ–‡æœ¬
            max_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°
            
        Returns:
            åŒ…å«çŠ¶æ€å’Œæè¿°çš„å­—å…¸
        """
        try:
            # ä½¿ç”¨é»˜è®¤æç¤ºå¦‚æœæ²¡æœ‰æä¾›
            if prompt is None:
                prompt = "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
            
            # å»¶è¿ŸåŠ è½½è§†è§‰æ¨¡å‹
            if self.vision_model is None and not self.model_loaded["vision"]:
                # éœ€è¦æ¸…ç†å†…å­˜ä»¥åŠ è½½æ–°æ¨¡å‹
                if self.text_model or self.image_gen_model:
                    self._clear_memory()
                self.vision_model, self.vision_tokenizer = self._load_vision_model()
                self.model_loaded["vision"] = True
            
            if not self.vision_model or not self.vision_tokenizer:
                return {
                    "status": "error",
                    "error": "è§†è§‰æ¨¡å‹åŠ è½½å¤±è´¥æˆ–ä¸å¯ç”¨"
                }
            
            # å¤„ç†å›¾åƒè¾“å…¥
            if isinstance(image, str):
                # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼ŒåŠ è½½å›¾åƒ
                if not os.path.exists(image):
                    return {
                        "status": "error",
                        "error": f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image}"
                    }
                image = Image.open(image).convert("RGB")
            elif not isinstance(image, Image.Image):
                return {
                    "status": "error",
                    "error": "æ— æ•ˆçš„å›¾åƒè¾“å…¥ï¼Œéœ€è¦PIL Imageå¯¹è±¡æˆ–å›¾åƒè·¯å¾„"
                }
            
            # å¤„ç†è¾“å…¥ï¼ˆæ ¹æ®æ¨¡å‹APIå¯èƒ½éœ€è¦è°ƒæ•´ï¼‰
            # è¿™é‡Œå‡è®¾æ¨¡å‹æ”¯æŒæ–‡æœ¬å’Œå›¾åƒä½œä¸ºè¾“å…¥
            # å®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´
            inputs = {
                "text": prompt,
                "images": image
            }
            
            # ç”Ÿæˆæè¿°
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹çš„APIè°ƒæ•´ç”Ÿæˆé€»è¾‘
            # ç”±äºä¸åŒè§†è§‰æ¨¡å‹APIå¯èƒ½ä¸åŒï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªé€šç”¨å®ç°
            with torch.no_grad():
                # è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®æ¨¡å‹çš„å…·ä½“APIè°ƒæ•´
                # å‡è®¾æ¨¡å‹æ¥å—input_idså’Œpixel_valuesä½œä¸ºè¾“å…¥
                # ç”±äºAutoModelForVision2Seqçš„å…·ä½“ä½¿ç”¨æ–¹å¼å¯èƒ½ä¸åŒï¼Œè¿™é‡Œåªæä¾›æ¡†æ¶
                try:
                    # å°è¯•ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæè¿°
                    # æ³¨æ„ï¼šè¿™éƒ¨åˆ†ä»£ç å¯èƒ½éœ€è¦æ ¹æ®å®é™…ä½¿ç”¨çš„è§†è§‰æ¨¡å‹è¿›è¡Œè°ƒæ•´
                    # è¿™é‡Œåªæ˜¯æä¾›ä¸€ä¸ªå‚è€ƒå®ç°
                    description = "ç¤ºä¾‹å›¾åƒæè¿°ï¼ˆå®é™…å®ç°éœ€è¦æ ¹æ®æ¨¡å‹APIè°ƒæ•´ï¼‰"
                except Exception as model_e:
                    logger.error(f"æ¨¡å‹æ¨ç†é”™è¯¯: {str(model_e)}")
                    return {
                        "status": "error",
                        "error": f"æ¨¡å‹æ¨ç†å¤±è´¥: {str(model_e)}"
                    }
            
            return {
                "status": "success",
                "response": description
            }
            
        except Exception as e:
            logger.error(f"æè¿°å›¾åƒæ—¶å‡ºé”™: {str(e)}")
            return {
                "status": "error",
                "error": f"é”™è¯¯: {str(e)}"
            }
    
    def generate_image(self, prompt, negative_prompt=None, height=512, width=512, num_inference_steps=20):
        """
        ä»æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒ
        
        Args:
            prompt: å›¾åƒç”Ÿæˆçš„æ–‡æœ¬æç¤º
            negative_prompt: å¯é€‰çš„è´Ÿé¢æç¤º
            height: å›¾åƒé«˜åº¦
            width: å›¾åƒå®½åº¦
            num_inference_steps: æ¨ç†æ­¥æ•°
            
        Returns:
            åŒ…å«çŠ¶æ€å’Œå›¾åƒçš„å­—å…¸
        """
        try:
            # å»¶è¿ŸåŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹
            if self.image_gen_model is None and not self.model_loaded["image_gen"]:
                # éœ€è¦æ¸…ç†å†…å­˜ä»¥åŠ è½½æ–°æ¨¡å‹
                if self.text_model or self.vision_model:
                    self._clear_memory()
                self.image_gen_model = self._load_image_gen_model()
                self.model_loaded["image_gen"] = True
            
            if not self.image_gen_model:
                return {
                    "status": "error",
                    "error": "å›¾åƒç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥æˆ–ä¸å¯ç”¨"
                }
            
            # ç”Ÿæˆå›¾åƒ
            with torch.no_grad():
                image = self.image_gen_model(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    height=height,
                    width=width,
                    num_inference_steps=num_inference_steps
                ).images[0]
            
            return {
                "status": "success",
                "image": image
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™: {str(e)}")
            return {
                "status": "error",
                "error": f"é”™è¯¯: {str(e)}"
            }
    
    def process_request(self, request_type, **kwargs):
        """
        é€šè¿‡ç»Ÿä¸€æ¥å£å¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚
        æ”¯æŒæ–‡æœ¬ç”Ÿæˆ(chat)ã€å›¾åƒæè¿°(describe_image)å’Œå›¾åƒç”Ÿæˆ(generate_image)
        
        Args:
            request_type: è¯·æ±‚ç±»å‹ ('chat', 'describe_image', 'generate_image')
            **kwargs: ç‰¹å®šè¯·æ±‚ç±»å‹çš„é™„åŠ å‚æ•°
            
        Returns:
            åŒ…å«çŠ¶æ€å’Œå“åº”/å›¾åƒçš„å­—å…¸
        """
        if request_type == "chat":
            return self.chat(**kwargs)
        elif request_type == "describe_image":
            return self.describe_image(**kwargs)
        elif request_type == "generate_image":
            return self.generate_image(**kwargs)
        else:
            return {
                "status": "error",
                "error": f"ä¸æ”¯æŒçš„è¯·æ±‚ç±»å‹: {request_type}"
            }
    
    def __del__(self):
        """
        ææ„å‡½æ•°ï¼Œæ¸…ç†æ‰€æœ‰èµ„æº
        """
        try:
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # é‡Šæ”¾æ‰€æœ‰æ¨¡å‹å¼•ç”¨
            self.text_model = None
            self.text_tokenizer = None
            self.vision_model = None
            self.vision_tokenizer = None
            self.image_gen_model = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {str(e)}")

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºé€‚é…å™¨å®ä¾‹
    adapter = ModelAdapter()
    
    # æµ‹è¯•æ–‡æœ¬å¯¹è¯
    response = adapter.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
    print(f"\nğŸ’¬ æ–‡æœ¬å¯¹è¯ç»“æœ:")
    print(response)