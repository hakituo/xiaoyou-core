#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æ¨¡å‹é€‚é…å™¨
ä¸“æ³¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„é€‚é…å™¨
"""

import os
import logging
import json
import time
import asyncio
import torch
import threading
import re
from typing import Dict, Optional, Any, Union, List
from .core_engine.model_manager import get_model_manager
from .utils.base_adapter import BaseAdapter
from .llm.dashscope_client import get_dashscope_client

logger = logging.getLogger(__name__)


class TextModelAdapter(BaseAdapter):
    """
    æ–‡æœ¬æ¨¡å‹é€‚é…å™¨
    å¤„ç†æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç­‰ä»»åŠ¡
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        # é»˜è®¤é…ç½®
        default_config = {
            'model_type': 'transformers',  # transformers, ollama, vllm, infer_service
            'text_model_path': '',
            'device': 'auto',
            'quantization': {
                'enabled': False,
                'load_in_8bit': False,
                'load_in_4bit': False,
                'torch_dtype': torch.float16 if torch.cuda.is_available() else torch.float32
            },
            'ollama_base_url': 'http://localhost:11434',
            'ollama_model': 'llama3',
            'vllm_base_url': 'http://localhost:8000/generate',
            'vllm_model': 'facebook/opt-125m',
            'timeout': 60,
            'max_retries': 3
        }
        
        # åˆå¹¶é…ç½®
        self.config = default_config.copy()
        if config:
            self.config.update(config)
        
        # è®¾ç½®æ¨¡å‹ç±»å‹
        self._model_type = self.config['model_type']
        
        # è®¾ç½®æ¨¡å‹åç§°
        model_name = f"text_{self._model_type}_{hash(self.config['text_model_path'])}" if self.config['text_model_path'] else f"text_{self._model_type}"
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(get_model_manager(), 'text', model_name)
        
        # æ³¨å†Œæ¨¡å‹åˆ°ç®¡ç†å™¨
        self._register_model()
        self._llama = None

    def _register_model(self):
        """æ³¨å†Œæ¨¡å‹åˆ°æ¨¡å‹ç®¡ç†å™¨"""
        try:
            # å³ä½¿text_model_pathä¸ºç©ºï¼Œä¹Ÿæ³¨å†Œæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„æˆ–å ä½ç¬¦
            model_path = self.config['text_model_path'] if self.config['text_model_path'] else 'default_model'
            success = self.model_manager.register_model(
                model_name=self._model_name,
                model_type='llm',
                model_path=model_path
            )
            if success:
                logger.info(f"æ–‡æœ¬æ¨¡å‹å·²æ³¨å†Œ: {self._model_name}")
            else:
                logger.warning(f"æ–‡æœ¬æ¨¡å‹æ³¨å†Œå¤±è´¥æˆ–å·²å­˜åœ¨: {self._model_name}")
                # å¦‚æœæ¨¡å‹å·²å­˜åœ¨ï¼Œç¡®ä¿é”å·²åˆ›å»º
                if self._model_name not in self.model_manager._model_locks:
                    self.model_manager._model_locks[self._model_name] = threading.Lock()
        except Exception as e:
            logger.error(f"æ³¨å†Œæ–‡æœ¬æ¨¡å‹å¤±è´¥: {str(e)}")
            # å³ä½¿å‡ºé”™ä¹Ÿè¦ç¡®ä¿é”å·²åˆ›å»º
            if not hasattr(self.model_manager, '_model_locks'):
                self.model_manager._model_locks = {}
            if self._model_name not in self.model_manager._model_locks:
                self.model_manager._model_locks[self._model_name] = threading.Lock()

    def _prepare_model_load_params(self) -> Dict[str, Any]:
        """
        å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
        
        Returns:
            Dict: æ¨¡å‹åŠ è½½å‚æ•°
        """
        if self._model_type != 'transformers':
            return {}
            
        load_kwargs = {
            'device': self.config['device'],
            'torch_dtype': self.config['quantization']['torch_dtype'],
            'quantized': self.config['quantization']['enabled'],
            'model_kwargs': {}
        }
        
        # æ·»åŠ é‡åŒ–å‚æ•°åˆ°model_kwargsä¸­ï¼Œé¿å…ä¸quantization_configå†²çª
        if self.config['quantization']['load_in_8bit']:
            load_kwargs['model_kwargs']['load_in_8bit'] = True
        elif self.config['quantization']['load_in_4bit']:
            load_kwargs['model_kwargs']['load_in_4bit'] = True
        q = self.config['quantization']
        load_kwargs['quantization_config'] = {
            'enabled': q.get('enabled', False),
            'load_in_4bit': q.get('load_in_4bit', False),
            'load_in_8bit': q.get('load_in_8bit', False),
            'bnb_4bit_quant_type': q.get('bnb_4bit_quant_type', 'nf4'),
            'bnb_4bit_compute_dtype': q.get('torch_dtype', q.get('bnb_4bit_compute_dtype', None)),
            'bnb_4bit_use_double_quant': q.get('bnb_4bit_use_double_quant', True),
            'bitsandbytes': True
        }
        
        return load_kwargs
        
    def load_model(self) -> bool:
        """
        åŠ è½½æ–‡æœ¬æ¨¡å‹
        
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            if self._model_type != 'transformers':
                self._is_loaded = True
                return True
            
            # ä½¿ç”¨åŸºç±»çš„åŠ è½½æ–¹æ³•
            return super().load_model()
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æœ¬æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            return False

    def _process_vision_inputs(self, messages):
        """
        å¤„ç†è§†è§‰è¾“å…¥ï¼Œæå–å›¾åƒå’Œè§†é¢‘
        """
        image_inputs = []
        video_inputs = []
        
        if not messages:
            return image_inputs, video_inputs
            
        for message in messages:
            if message.get("role") == "user":
                content = message.get("content")
                if isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict):
                            if item.get("type") == "image":
                                image_inputs.append(item.get("image"))
                            elif item.get("type") == "image_url":
                                # å¤„ç† image_url æ ¼å¼ (OpenAIå…¼å®¹)
                                url = item.get("image_url", {}).get("url", "") if isinstance(item.get("image_url"), dict) else item.get("image_url", "")
                                if url.startswith("data:image"):
                                    # base64 handling could be added here if processor supports it directly
                                    # or convert to PIL Image
                                    pass
                                image_inputs.append(url) # AutoProcessor usually handles URLs
        return image_inputs, video_inputs

    def _chat_with_vision_model(self, model, processor, messages, prompt, max_tokens, temperature, top_p):
        """
        ä½¿ç”¨è§†è§‰æ¨¡å‹ç”Ÿæˆå“åº”
        """
        try:
            logger.info("ğŸ‘ï¸ ä½¿ç”¨è§†è§‰æ¨¡å‹ç”Ÿæˆå“åº”")
            
            # æ„é€  Qwen2-VL æ ¼å¼çš„æ¶ˆæ¯
            # ç¡®ä¿ prompt æ˜¯å­—ç¬¦ä¸² (å¦‚æœä¸æ˜¯ï¼Œè¯´æ˜å·²ç»åœ¨ messages é‡Œå¤„ç†äº†)
            
            qwen_messages = []
            
            if messages:
                # è½¬æ¢ messages æ ¼å¼
                for msg in messages:
                    role = msg.get("role", "user")
                    content = msg.get("content")
                    
                    new_content = []
                    if isinstance(content, str):
                        new_content.append({"type": "text", "text": content})
                    elif isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict):
                                if item.get("type") == "text":
                                    new_content.append({"type": "text", "text": item.get("text", "")})
                                elif item.get("type") in ["image", "image_url"]:
                                    # å‡è®¾ processor èƒ½å¤„ç† url æˆ– PIL Image
                                    # è¿™é‡Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ image æ˜¯æ­£ç¡®çš„æ ¼å¼
                                    img_val = item.get("image") or item.get("image_url")
                                    if isinstance(img_val, dict) and "url" in img_val:
                                        img_val = img_val["url"]
                                    new_content.append({"type": "image", "image": img_val})
                    
                    qwen_messages.append({"role": role, "content": new_content})
            else:
                # å¦‚æœåªæœ‰ prompt
                qwen_messages.append({
                    "role": "user", 
                    "content": [{"type": "text", "text": str(prompt)}]
                })

            # å‡†å¤‡è¾“å…¥
            text = processor.apply_chat_template(
                qwen_messages, tokenize=False, add_generation_prompt=True
            )
            
            # æå–å›¾åƒè¾“å…¥
            image_inputs, video_inputs = self._process_vision_inputs(qwen_messages)
            
            # å¤„ç†è¾“å…¥
            inputs = processor(
                text=[text],
                images=image_inputs if image_inputs else None,
                videos=video_inputs if video_inputs else None,
                padding=True,
                return_tensors="pt"
            )
            
            # ç§»è‡³è®¾å¤‡
            device = next(model.parameters()).device
            inputs = inputs.to(device)
            
            # ç”Ÿæˆ
            generated_ids = model.generate(
                **inputs, 
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            # è§£ç 
            generated_ids_trimmed = [
                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            return output_text[0] if output_text else ""
            
        except Exception as e:
            logger.error(f"è§†è§‰æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            raise e

    def _clear_memory(self):
        """æ¸…ç†æ˜¾å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        import gc
        gc.collect()

    def stream_chat(self, 
                    messages: Optional[List[Dict[str, Any]]] = None,
                    prompt: Optional[Union[str, List, Dict]] = None,
                    max_tokens: int = 512, 
                    temperature: float = 0.7, 
                    top_p: float = 0.9,
                    stop_phrases: Optional[List[str]] = None,
                    **kwargs) -> Any:
        """
        æµå¼ç”Ÿæˆå¯¹è¯å“åº”
        Returns:
            Generator yielding response chunks
        """
        try:
            # å…¼å®¹messageså’Œpromptä¸¤ç§å‚æ•°æ ¼å¼
            if messages is not None and isinstance(messages, list) and len(messages) > 0:
                user_messages = [msg['content'] for msg in messages if msg.get('role') == 'user']
                if user_messages:
                    prompt = user_messages[-1]
                else:
                    prompt = messages[-1]['content']
            elif prompt is None:
                yield {"status": "error", "error": "å¿…é¡»æä¾›messagesæˆ–promptå‚æ•°"}
                return
            
            if not prompt and not messages:
                 yield {"status": "error", "error": "å¿…é¡»æä¾›messagesæˆ–promptå‚æ•°"}
                 return
            
            if not self._ensure_model_loaded():
                if not self.load_model():
                    yield {"status": "error", "error": "æ¨¡å‹åŠ è½½å¤±è´¥"}
                    return
            
            actual_max = max_tokens if isinstance(max_tokens, int) and max_tokens > 0 else 1024
            
            if self._model_type == 'transformers':
                for chunk in self._stream_chat_with_transformers(prompt, actual_max, temperature, top_p, messages, stop_phrases):
                    yield chunk
            elif self._model_type == 'ollama':
                for chunk in self._stream_chat_with_ollama(prompt, actual_max, temperature, top_p):
                    yield chunk
            elif self._model_type == 'vllm':
                for chunk in self._stream_chat_with_vllm(prompt, actual_max, temperature, top_p):
                    yield chunk
            elif self._model_type == 'dashscope':
                response = self._chat_with_dashscope(prompt, actual_max, temperature, top_p, messages)
                yield response
            elif self._model_type == 'infer_service':
                # æ¨ç†æœåŠ¡æš‚ä¸æ”¯æŒæµå¼
                response = self._chat_with_infer_service(prompt, actual_max, temperature, top_p)
                yield response
            elif self._model_type == 'llama_cpp':
                for chunk in self._stream_chat_with_llama_cpp(prompt, actual_max, temperature, top_p, messages):
                    yield chunk
            else:
                yield {"status": "error", "error": f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self._model_type}"}
                
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            yield {"status": "error", "error": f"ç”Ÿæˆé”™è¯¯: {str(e)}"}

    def _stream_chat_with_transformers(self, 
                              prompt: str, 
                              max_tokens: int, 
                              temperature: float, 
                              top_p: float,
                              messages: Optional[List[Dict[str, str]]] = None,
                              stop_phrases: Optional[List[str]] = None):
        """
        ä½¿ç”¨Transformersæ¨¡å‹æµå¼ç”Ÿæˆå“åº”
        """
        from transformers import TextIteratorStreamer
        from threading import Thread
        
        model = self.model_manager.get_model(self._model_name)
        tokenizer = self.model_manager.get_tokenizer(self._model_name)
        
        if not model or not tokenizer:
            yield "æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåŠ è½½"
            return

        device = next(model.parameters()).device
        
        # æ„é€ è¾“å…¥
        chat_messages = messages
        if chat_messages is None and hasattr(tokenizer, 'apply_chat_template'):
             # ç®€å•çš„ prompt è½¬ message é€»è¾‘ (çœç•¥å¤æ‚è§£æ)
             chat_messages = [{"role": "user", "content": prompt}]
             
        if chat_messages is not None and hasattr(tokenizer, 'apply_chat_template'):
            try:
                input_ids = tokenizer.apply_chat_template(chat_messages, add_generation_prompt=True, return_tensors='pt')
                input_ids = input_ids.to(device)
            except Exception:
                inputs = tokenizer(prompt, return_tensors='pt')
                input_ids = inputs.input_ids.to(device)
        else:
            inputs = tokenizer(prompt, return_tensors='pt')
            input_ids = inputs.input_ids.to(device)
            
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        generation_kwargs = dict(
            input_ids=input_ids,
            streamer=streamer,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        for new_text in streamer:
            yield new_text

    def _stream_chat_with_llama_cpp(self,
                             prompt: str,
                             max_tokens: int,
                             temperature: float,
                             top_p: float,
                             messages: Optional[List[Dict[str, str]]] = None):
        """
        ä½¿ç”¨llama_cppæµå¼ç”Ÿæˆå“åº”
        """
        if self._llama is None:
            # å°è¯•åˆå§‹åŒ– (å¤ç”¨ _chat_with_llama_cpp çš„é€»è¾‘ï¼Œè¿™é‡Œç®€åŒ–)
            try:
                self._chat_with_llama_cpp(prompt, max_tokens, temperature, top_p, messages) # è§¦å‘åˆå§‹åŒ–
            except Exception:
                yield "æ¨¡å‹åˆå§‹åŒ–å¤±è´¥"
                return

        # ä¼˜å…ˆä½¿ç”¨ chat completion æ¥å£
        if messages and hasattr(self._llama, 'create_chat_completion'):
            stop_tokens = ["User:", "user:", "\nUser", "<|user|>", "<|end|>", "<|endoftext|>", "\n\n\n"]
            valid_messages = [m for m in messages if isinstance(m, dict) and 'role' in m and 'content' in m]
            
            stream = self._llama.create_chat_completion(
                messages=valid_messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=stop_tokens,
                stream=True
            )
            
            for chunk in stream:
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    delta = chunk['choices'][0].get('delta', {})
                    if 'content' in delta:
                        yield delta['content']
        else:
            # å›é€€åˆ° text completion
            stop_tokens = ["User:", "user:", "\nUser", "<|user|>", "<|end|>", "<|endoftext|>"]
            stream = self._llama.create_completion(
                prompt=prompt, 
                max_tokens=max_tokens, 
                temperature=temperature, 
                top_p=top_p,
                stop=stop_tokens,
                stream=True
            )
            for chunk in stream:
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    text = chunk['choices'][0].get('text', '')
                    yield text

    def _stream_chat_with_ollama(self, 
                         prompt: str, 
                         max_tokens: int, 
                         temperature: float, 
                         top_p: float) -> str:
        """
        ä½¿ç”¨Ollama APIæµå¼ç”Ÿæˆå“åº”
        """
        try:
            import requests
            import json
            
            url = f"{self.config['ollama_base_url']}/generate"
            model_name = self.config.get("ollama_model", "llama3")
            
            payload = {
                "model": model_name,
                "prompt": prompt,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p
                },
                "stream": True
            }
            
            headers = {"Content-Type": "application/json"}
            
            with requests.post(url, json=payload, headers=headers, stream=True) as response:
                if response.status_code == 200:
                    for line in response.iter_lines():
                        if line:
                            data = json.loads(line)
                            if 'response' in data:
                                yield data['response']
                            if data.get('done', False):
                                break
                else:
                    yield f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code}"
        except Exception as e:
             yield f"Ollamaå¤„ç†é”™è¯¯: {str(e)}"

    def _stream_chat_with_vllm(self, 
                        prompt: str, 
                        max_tokens: int, 
                        temperature: float, 
                        top_p: float) -> str:
        """
        ä½¿ç”¨vLLM APIæµå¼ç”Ÿæˆå“åº”
        """
        try:
            import requests
            import json
            
            url = self.config["vllm_base_url"]
            model_name = self.config.get("vllm_model", "facebook/opt-125m")
            
            payload = {
                "prompt": prompt,
                "model": model_name,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stream": True
            }
            
            headers = {"Content-Type": "application/json"}
            
            with requests.post(url, json=payload, headers=headers, stream=True) as response:
                if response.status_code == 200:
                    for line in response.iter_lines():
                        if line:
                            # vLLM SSE format: data: {...}
                            decoded_line = line.decode('utf-8')
                            if decoded_line.startswith("data: "):
                                data_str = decoded_line[6:]
                                if data_str == "[DONE]":
                                    break
                                try:
                                    data = json.loads(data_str)
                                    # vLLM returns full text in each chunk usually? No, depends on config.
                                    # Usually choices[0].text or delta
                                    if 'choices' in data:
                                        choice = data['choices'][0]
                                        if 'text' in choice:
                                            # vLLM might return accumulated text, need to check docs.
                                            # Assuming standard OpenAI compatible or vLLM native which might be accumulated.
                                            # For simplicity, let's assume it returns diff or handle it.
                                            # Actually vLLM native /generate returns list of strings.
                                            # Let's assume standard OpenAI compatible /v1/completions if using vLLM as server.
                                            # But url defaults to /generate.
                                            # vLLM /generate stream returns: {"text": ["..."], ...}
                                            text = choice['text']
                                            # This might be full text. 
                                            # TODO: Handle vLLM streaming correctly. For now yield text.
                                            yield text
                                except:
                                    pass
                else:
                     yield f"vLLM APIè°ƒç”¨å¤±è´¥: {response.status_code}"
        except Exception as e:
             yield f"vLLMå¤„ç†é”™è¯¯: {str(e)}"

    def chat(self, 
             messages: Optional[List[Dict[str, Any]]] = None,
             prompt: Optional[Union[str, List, Dict]] = None,
             max_tokens: int = 512, 
             temperature: float = 0.7, 
             top_p: float = 0.9,
             stop_phrases: Optional[List[str]] = None,
             **kwargs) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ–‡æœ¬æ¨¡å‹ç”Ÿæˆå¯¹è¯å“åº” (å¸¦é‡è¯•æœºåˆ¶)
        """
        try:
            # å…¼å®¹messageså’Œpromptä¸¤ç§å‚æ•°æ ¼å¼
            if messages is not None and isinstance(messages, list) and len(messages) > 0:
                user_messages = [msg['content'] for msg in messages if msg.get('role') == 'user']
                if user_messages:
                    prompt = user_messages[-1]
                else:
                    prompt = messages[-1]['content']
            elif prompt is None:
                return {"status": "error", "error": "å¿…é¡»æä¾›messagesæˆ–promptå‚æ•°"}
            
            if not prompt and not messages:
                 return {"status": "error", "error": "å¿…é¡»æä¾›messagesæˆ–promptå‚æ•°"}
            
            self._performance_tracker.start_tracking()
            
            if not self._ensure_model_loaded():
                if not self.load_model():
                    return {"status": "error", "error": "æ¨¡å‹åŠ è½½å¤±è´¥"}
            
            actual_max = max_tokens if isinstance(max_tokens, int) and max_tokens > 0 else 1024
            
            # é‡è¯•é€»è¾‘
            max_retries = self.config.get('max_retries', 3)
            retry_count = 0
            last_error = None
            
            while retry_count <= max_retries:
                try:
                    if self._model_type == 'transformers':
                        response = self._chat_with_transformers(prompt, actual_max, temperature, top_p, messages, stop_phrases)
                    elif self._model_type == 'ollama':
                        response = self._chat_with_ollama(prompt, actual_max, temperature, top_p)
                    elif self._model_type == 'vllm':
                        response = self._chat_with_vllm(prompt, actual_max, temperature, top_p)
                    elif self._model_type == 'infer_service':
                        response = self._chat_with_infer_service(prompt, actual_max, temperature, top_p)
                    elif self._model_type == 'dashscope':
                        response = self._chat_with_dashscope(prompt, actual_max, temperature, top_p, messages)
                    elif self._model_type == 'llama_cpp':
                        response = self._chat_with_llama_cpp(prompt, actual_max, temperature, top_p, messages)
                    else:
                        return {"status": "error", "error": f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self._model_type}"}
                    
                    self._performance_tracker.end_tracking()
                    return {"status": "success", "response": response}
                    
                except (RuntimeError, Exception) as e:
                    last_error = e
                    error_str = str(e)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å­˜é”™è¯¯ (OOM)
                    is_oom = "out of memory" in error_str.lower() or "allocate" in error_str.lower() or isinstance(e, MemoryError)
                    
                    if is_oom:
                        logger.warning(f"æ£€æµ‹åˆ°OOMé”™è¯¯ (å°è¯• {retry_count+1}/{max_retries+1}): {error_str}")
                        self._clear_memory()
                        # å°è¯•å‡å°‘max_tokens
                        if actual_max > 128:
                            actual_max = int(actual_max * 0.7)
                            logger.info(f"å‡å°‘max_tokensè‡³: {actual_max}")
                    else:
                        logger.warning(f"ç”Ÿæˆå‡ºé”™ (å°è¯• {retry_count+1}/{max_retries+1}): {error_str}")
                    
                    retry_count += 1
                    if retry_count <= max_retries:
                        time.sleep(1 * retry_count)  # æŒ‡æ•°é€€é¿
                    else:
                        break

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            self._performance_tracker.end_tracking(error_occurred=True)
            logger.error(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {str(last_error)}")
            return {"status": "error", "error": f"ç”Ÿæˆå¤±è´¥: {str(last_error)}"}
            
        except Exception as e:
            self._performance_tracker.end_tracking(error_occurred=True)
            logger.error(f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            return {"status": "error", "error": f"ç”Ÿæˆé”™è¯¯: {str(e)}"}

    def generate(self,
                 prompt: Optional[str] = None,
                 messages: Optional[List[Dict[str, str]]] = None,
                 max_tokens: int = 512,
                 temperature: float = 0.7,
                 top_p: float = 0.9) -> Dict[str, Any]:
        try:
            res = self.chat(messages=messages, prompt=prompt, max_tokens=max_tokens, temperature=temperature, top_p=top_p)
            if res.get("status") == "success":
                return {"status": "success", "data": {"text": res.get("response", "")}}
            return {"status": "error", "error": res.get("error", "ç”Ÿæˆå¤±è´¥")}
        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _chat_with_transformers(self, 
                              prompt: str, 
                              max_tokens: int, 
                              temperature: float, 
                              top_p: float,
                              messages: Optional[List[Dict[str, str]]] = None,
                              stop_phrases: Optional[List[str]] = None) -> str:
        """
        ä½¿ç”¨æœ¬åœ°Transformersæ¨¡å‹ç”Ÿæˆå“åº”
        """
        logger.info(f"å‡†å¤‡ä½¿ç”¨transformersæ¨¡å‹ç”Ÿæˆå“åº”ï¼Œæ¨¡å‹åç§°: {self._model_name}")
        
        # è·å–æ¨¡å‹å’Œåˆ†è¯å™¨
        model = self.model_manager.get_model(self._model_name)
        tokenizer = self.model_manager.get_tokenizer(self._model_name)
        
        logger.info(f"è·å–æ¨¡å‹ç»“æœ: model={model is not None}, tokenizer={tokenizer is not None}")
        
        if not model:
            raise Exception(f"æ¨¡å‹æœªåŠ è½½: {self._model_name}")
        if not tokenizer:
            raise Exception(f"åˆ†è¯å™¨æœªåŠ è½½: {self._model_name}")
            
        # æ£€æŸ¥æ˜¯å¦ä¸º Vision Processor (é’ˆå¯¹ Qwen2-VL ç­‰)
        if hasattr(tokenizer, 'image_processor') or 'Processor' in str(type(tokenizer)):
             return self._chat_with_vision_model(model, tokenizer, messages, prompt, max_tokens, temperature, top_p)
        
        try:
            device = next(model.parameters()).device
            use_chat = False
            chat_messages = messages if isinstance(messages, list) and messages else None
            if chat_messages is None and hasattr(tokenizer, 'apply_chat_template'):
                try:
                    lines = str(prompt or '').splitlines()
                    sys_buf = []
                    parsed = []
                    name = None
                    for ln in lines:
                        if not ln:
                            continue
                        m_user = re.match(r'^\s*ç”¨æˆ·\s*:\s*(.*)$', ln)
                        if m_user:
                            if sys_buf:
                                parsed.append({'role': 'system', 'content': '\n'.join(sys_buf).strip()})
                                sys_buf = []
                            parsed.append({'role': 'user', 'content': m_user.group(1).strip()})
                            continue
                        m_asst = re.match(r'^\s*([^:]+)\s*:\s*(.*)$', ln)
                        if m_asst and m_asst.group(1).strip() != 'ç”¨æˆ·':
                            name = m_asst.group(1).strip()
                            content = m_asst.group(2).strip()
                            if content:
                                parsed.append({'role': 'assistant', 'content': content})
                            continue
                        sys_buf.append(ln)
                    if sys_buf:
                        parsed.insert(0, {'role': 'system', 'content': '\n'.join(sys_buf).strip()})
                    parsed = [m for m in parsed if isinstance(m, dict) and m.get('content')]
                    if any(m.get('role') == 'user' for m in parsed):
                        chat_messages = parsed
                except Exception:
                    chat_messages = None
            if chat_messages is not None and hasattr(tokenizer, 'apply_chat_template'):
                try:
                    input_ids = tokenizer.apply_chat_template(chat_messages, add_generation_prompt=True, return_tensors='pt')
                    input_ids = input_ids.to(device)
                    stopping = None
                    if stop_phrases:
                        from transformers import StoppingCriteria, StoppingCriteriaList
                        class PhraseStop(StoppingCriteria):
                            def __init__(self, phrases_ids):
                                super().__init__()
                                self.phrases_ids = phrases_ids
                            def __call__(self, input_ids, scores, **kwargs):
                                seq = input_ids[0].tolist()
                                for p in self.phrases_ids:
                                    L = len(p)
                                    if L > 0 and len(seq) >= L and seq[-L:] == p:
                                        return True
                                return False
                        phrases_ids = []
                        for s in stop_phrases:
                            try:
                                ids = tokenizer(s, add_special_tokens=False, return_tensors='pt').input_ids[0].tolist()
                                if ids:
                                    phrases_ids.append(ids)
                            except Exception:
                                pass
                        if phrases_ids:
                            stopping = StoppingCriteriaList([PhraseStop(phrases_ids)])
                    with torch.no_grad():
                        output = model.generate(
                            input_ids=input_ids,
                            max_new_tokens=max_tokens,
                            temperature=temperature,
                            top_p=top_p,
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            stopping_criteria=stopping
                        )
                    gen_ids = output[0][input_ids.shape[-1]:]
                    response = tokenizer.decode(gen_ids, skip_special_tokens=True)
                    return response.strip()
                except Exception:
                    pass
            inputs = tokenizer(prompt, return_tensors='pt')
            inputs = {k: v.to(device) for k, v in inputs.items()}
            stopping = None
            if stop_phrases:
                from transformers import StoppingCriteria, StoppingCriteriaList
                class PhraseStop(StoppingCriteria):
                    def __init__(self, phrases_ids):
                        super().__init__()
                        self.phrases_ids = phrases_ids
                    def __call__(self, input_ids, scores, **kwargs):
                        seq = input_ids[0].tolist()
                        for p in self.phrases_ids:
                            L = len(p)
                            if L > 0 and len(seq) >= L and seq[-L:] == p:
                                return True
                        return False
                phrases_ids = []
                for s in stop_phrases:
                    try:
                        ids = tokenizer(s, add_special_tokens=False, return_tensors='pt').input_ids[0].tolist()
                        if ids:
                            phrases_ids.append(ids)
                    except Exception:
                        pass
                if phrases_ids:
                    stopping = StoppingCriteriaList([PhraseStop(phrases_ids)])
            with torch.no_grad():
                output = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    stopping_criteria=stopping
                )
            response = tokenizer.decode(output[0], skip_special_tokens=True)
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            elif prompt in response:
                parts = response.split(prompt)
                if len(parts) > 1:
                    response = parts[-1].strip()
            unwanted_starters = ['å§ã€‚', 'çš„ã€‚', 'äº†ã€‚', 'å‘¢ã€‚', 'å•Šã€‚', 'ï¼', 'ï¼Ÿ', 'ã€‚', 'ï¼Œ', 'ï¼š', 'ï¼›']
            for starter in unwanted_starters:
                if response.startswith(starter):
                    response = response[len(starter):].strip()
                    break
            return response
        except Exception as e:
            logger.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}", exc_info=True)
            raise

    def _chat_with_llama_cpp(self,
                             prompt: str,
                             max_tokens: int,
                             temperature: float,
                             top_p: float,
                             messages: Optional[List[Dict[str, str]]] = None) -> str:
        try:
            from llama_cpp import Llama
        except ImportError:
            raise Exception("æœªå®‰è£… llama_cpp æ¨¡å—ï¼Œè¯·è¿è¡Œ pip install llama-cpp-python")
        except Exception as e:
            raise Exception(f"å¯¼å…¥ llama_cpp å¤±è´¥: {str(e)}")

        if self._llama is None:
            model_path = self.config.get('text_model_path') or ''
            if not model_path:
                raise Exception('ç¼ºå°‘GGUFæ¨¡å‹è·¯å¾„')
            
            # å°è¯•é‡Šæ”¾ PyTorch æ˜¾å­˜ï¼Œä¸º llama_cpp è…¾å‡ºç©ºé—´
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # è·å–é…ç½®å‚æ•°
            n_gpu_layers = int(self.config.get('n_gpu_layers', -1))
            n_batch = int(self.config.get('n_batch', 256))  # é™ä½é»˜è®¤ batch size ä»¥èŠ‚çœæ˜¾å­˜
            # é™ä½é»˜è®¤ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé˜²æ­¢ OOM
            n_ctx_default = 2048
            if max_tokens * 4 > n_ctx_default:
                n_ctx_default = max_tokens * 4
            n_ctx = int(self.config.get('n_ctx', n_ctx_default))
            
            logger.info(f"åˆå§‹åŒ– llama_cpp: n_gpu_layers={n_gpu_layers}, n_ctx={n_ctx}, n_batch={n_batch}")
            
            # å°è¯•é‡Šæ”¾æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            self._llama = Llama(
                model_path=model_path, 
                n_ctx=n_ctx, 
                n_threads=8, 
                n_batch=n_batch, 
                n_gpu_layers=n_gpu_layers,
                verbose=False  # ç¦ç”¨åº•å±‚è¯¦ç»†æ—¥å¿—ï¼Œé˜²æ­¢ç»ˆç«¯é˜»å¡
            )
        
        # æ¯æ¬¡ç”Ÿæˆå‰æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        # ä¼˜å…ˆä½¿ç”¨ chat completion æ¥å£
        if messages and hasattr(self._llama, 'create_chat_completion'):
            try:
                # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®
                valid_messages = []
                for msg in messages:
                    if isinstance(msg, dict) and 'role' in msg and 'content' in msg:
                        valid_messages.append(msg)
                
                if valid_messages:
                    logger.info("ä½¿ç”¨llama_cpp create_chat_completionæ¥å£")
                    
                    # æ„é€ åœæ­¢è¯åˆ—è¡¨
                    stop_tokens = ["User:", "user:", "\nUser", "<|user|>", "<|end|>", "<|endoftext|>", "\n\n\n"]
                    
                    out = self._llama.create_chat_completion(
                        messages=valid_messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        stop=stop_tokens
                    )
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    return out['choices'][0]['message']['content'].strip()
            except Exception as e:
                error_str = str(e)
                logger.warning(f"llama_cpp chat completion å¤±è´¥: {error_str}")
                
                # å¦‚æœæ˜¯ä¸¥é‡çš„å†…å­˜è®¿é—®é”™è¯¯ï¼Œä¸è¦å›é€€ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸å¹¶é‡ç½®æ¨¡å‹
                if "access violation" in error_str.lower() or "segmentation fault" in error_str.lower():
                    logger.error("æ£€æµ‹åˆ°ä¸¥é‡æ¨¡å‹é”™è¯¯ï¼Œé‡ç½®æ¨¡å‹å®ä¾‹")
                    self._llama = None
                    raise Exception(f"æ¨¡å‹å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œå·²é‡ç½®: {error_str}")
                
                logger.warning("å°è¯•å›é€€åˆ° text completion")
        
        # å›é€€åˆ° text completion
        stop_tokens = ["User:", "user:", "\nUser", "<|user|>", "<|end|>", "<|endoftext|>"]
        out = self._llama.create_completion(
            prompt=prompt, 
            max_tokens=max_tokens, 
            temperature=temperature, 
            top_p=top_p,
            stop=stop_tokens
        )
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        txt = ''
        ch = out.get('choices')
        if isinstance(ch, list) and ch:
            txt = str(ch[0].get('text', ''))
        return txt.strip()

    def _chat_with_ollama(self, 
                         prompt: str, 
                         max_tokens: int, 
                         temperature: float, 
                         top_p: float) -> str:
        """
        ä½¿ç”¨Ollama APIç”Ÿæˆå“åº”
        """
        try:
            import requests
            
            url = f"{self.config['ollama_base_url']}/generate"
            model_name = self.config.get("ollama_model", "llama3")
            
            payload = {
                "model": model_name,
                "prompt": prompt,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p
                },
                "stream": False
            }
            
            headers = {"Content-Type": "application/json"}
            timeout = self.config.get("timeout", 60)
            
            logger.info(f"è°ƒç”¨Ollama API: {url}, æ¨¡å‹: {model_name}")
            response = requests.post(url, json=payload, headers=headers, timeout=timeout)
            
            if response.status_code == 200:
                data = response.json()
                return data.get("response", "")
            else:
                error_msg = f"Ollama APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}, {response.text}"
                logger.error(error_msg)
                raise Exception(error_msg)
                
        except ImportError:
            raise Exception("éœ€è¦å®‰è£…requestsåº“")
        except Exception as e:
            logger.error(f"Ollamaå¤„ç†é”™è¯¯: {str(e)}")
            raise Exception(f"Ollamaå¤„ç†é”™è¯¯: {str(e)}")

    def _chat_with_vllm(self, 
                        prompt: str, 
                        max_tokens: int, 
                        temperature: float, 
                        top_p: float) -> str:
        """
        ä½¿ç”¨vLLM APIç”Ÿæˆå“åº”
        """
        try:
            import requests
            
            url = self.config["vllm_base_url"]
            model_name = self.config.get("vllm_model", "facebook/opt-125m")
            
            payload = {
                "prompt": prompt,
                "model": model_name,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "skip_special_tokens": True
            }
            
            headers = {"Content-Type": "application/json"}
            timeout = self.config.get("timeout", 60)
            
            logger.info(f"è°ƒç”¨vLLM API: {url}, æ¨¡å‹: {model_name}")
            response = requests.post(url, json=payload, headers=headers, timeout=timeout)
            
            if response.status_code == 200:
                data = response.json()
                return data.get("text", "")
            else:
                error_msg = f"vLLM APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}, {response.text}"
                logger.error(error_msg)
                raise Exception(error_msg)
                
        except ImportError:
            raise Exception("éœ€è¦å®‰è£…requestsåº“")
        except Exception as e:
            logger.error(f"vLLMå¤„ç†é”™è¯¯: {str(e)}")
            raise Exception(f"vLLMå¤„ç†é”™è¯¯: {str(e)}")

    def _chat_with_dashscope(self,
                            prompt: str,
                            max_tokens: int,
                            temperature: float,
                            top_p: float,
                            messages: Optional[List[Dict[str, str]]] = None) -> str:
        """
        ä½¿ç”¨DashScope (Qwen) ç”Ÿæˆå“åº”
        """
        try:
            client = get_dashscope_client()
            
            # DashScope generate is async, so we need to run it
            # If we are in an async loop, this might fail with "asyncio.run() cannot be called from a running event loop"
            # But TextModelAdapter methods are sync. 
            # If called from async context (like FastAPI), we should ideally use await, but this method is sync.
            # For now, we assume this is called in a thread pool or we use a workaround.
            
            # However, _chat_with_infer_service uses asyncio.run() which implies this adapter 
            # is expected to be run in a way that allows it (e.g. thread pool).
            
            # Check if there is a running loop
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None
                
            if loop and loop.is_running():
                # This is tricky. We are in a sync method called from async context?
                # Or this sync method is called in run_in_threadpool.
                # If run_in_threadpool, there is no loop in that thread usually.
                # So asyncio.run() works.
                # But if called directly from async function without await/executor, it fails.
                # Given FastAPI structure, it's likely run_in_threadpool or blocked.
                
                # For safety, we can try to use the client's sync method if it had one, but it is async only.
                # We'll assume thread pool usage (standard for sync methods in FastAPI).
                future = asyncio.run_coroutine_threadsafe(client.generate(
                    prompt=prompt,
                    history=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p
                ), loop)
                result = future.result()
            else:
                result = asyncio.run(client.generate(
                    prompt=prompt,
                    history=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p
                ))
            
            if result.get("status") == "success":
                return result.get("text", "")
            else:
                raise Exception(result.get("error", "Unknown error"))
                
        except Exception as e:
            logger.error(f"DashScopeå¤„ç†é”™è¯¯: {str(e)}")
            raise Exception(f"DashScopeå¤„ç†é”™è¯¯: {str(e)}")

    def _chat_with_infer_service(self, 
                                prompt: str, 
                                max_tokens: int, 
                                temperature: float, 
                                top_p: float) -> str:
        """
        ä½¿ç”¨æ¨ç†æœåŠ¡ç”Ÿæˆå“åº”
        """
        try:
            # å°è¯•å¯¼å…¥æ¨ç†æœåŠ¡å®¢æˆ·ç«¯
            from .llm.infer_service_client import get_infer_client
            INFER_SERVICE_AVAILABLE = True
        except ImportError:
            INFER_SERVICE_AVAILABLE = False
            raise Exception("æ¨ç†æœåŠ¡å®¢æˆ·ç«¯ä¸å¯ç”¨")
        
        try:
            # åˆå§‹åŒ–æ¨ç†æœåŠ¡å®¢æˆ·ç«¯
            infer_client = get_infer_client()
            
            # è°ƒç”¨å¼‚æ­¥æ–¹æ³•ç”Ÿæˆå“åº”
            result = asyncio.run(infer_client.generate(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            ))
            
            # è§£æç»“æœ
            return result.get("text", "")
            
        except Exception as e:
            logger.error(f"æ¨ç†æœåŠ¡å¤„ç†é”™è¯¯: {str(e)}")
            raise Exception(f"æ¨ç†æœåŠ¡å¤„ç†é”™è¯¯: {str(e)}")

    def unload(self) -> bool:
        """
        å¸è½½æ¨¡å‹
        
        Returns:
            bool: æ˜¯å¦å¸è½½æˆåŠŸ
        """
        return self.unload_model()

    def health_check(self) -> Dict[str, Any]:
        """
        å¥åº·æ£€æŸ¥
        
        Returns:
            å¥åº·çŠ¶æ€ä¿¡æ¯
        """
        try:
            result = {
                "status": "healthy",
                "model_type": self._model_type,
                "timestamp": time.time()
            }
            
            if self._model_type == 'transformers':
                # æ£€æŸ¥æœ¬åœ°æ¨¡å‹
                result["model_loaded"] = self.is_loaded
                result["model_path"] = self.config["text_model_path"]
                
                if not self.is_loaded:
                    result["status"] = "warning"
                    result["message"] = "æ¨¡å‹æœªåŠ è½½ï¼Œä½†å¯ä»¥æŒ‰éœ€åŠ è½½"
                    
            elif self._model_type == 'ollama':
                # æµ‹è¯•Ollama APIè¿æ¥
                try:
                    import requests
                    url = f"{self.config['ollama_base_url']}/tags"
                    response = requests.get(url, timeout=5)
                    if response.status_code == 200:
                        result["ollama_connected"] = True
                        result["ollama_models"] = [model["name"] for model in response.json().get("models", [])]
                    else:
                        result["status"] = "unhealthy"
                        result["error"] = f"Ollama APIè¿æ¥å¤±è´¥: HTTP {response.status_code}"
                        result["ollama_connected"] = False
                except Exception as e:
                    result["status"] = "unhealthy"
                    result["error"] = f"Ollamaè¿æ¥é”™è¯¯: {str(e)}"
                    result["ollama_connected"] = False
                    
            elif self._model_type == 'vllm':
                # æµ‹è¯•vLLM APIè¿æ¥
                try:
                    # ä½¿ç”¨ç®€å•çš„è¯·æ±‚æµ‹è¯•vLLMè¿æ¥
                    test_prompt = "Hello, are you working?"
                    test_response = self._chat_with_vllm(test_prompt, 10, 0.7, 0.9)
                    result["vllm_connected"] = True
                    result["test_response"] = test_response[:50] + "..." if len(test_response) > 50 else test_response
                except Exception as e:
                    result["status"] = "unhealthy"
                    result["error"] = f"vLLMè¿æ¥é”™è¯¯: {str(e)}"
                    result["vllm_connected"] = False
                    
            elif self._model_type == 'infer_service':
                # æµ‹è¯•æ¨ç†æœåŠ¡è¿æ¥
                try:
                    from .llm.infer_service_client import get_infer_client
                    infer_client = get_infer_client()
                    health_result = asyncio.run(infer_client.health_check())
                    result["infer_service_connected"] = True
                    result["health_status"] = health_result.get("status", "unknown")
                except Exception as e:
                    result["status"] = "unhealthy"
                    result["error"] = f"æ¨ç†æœåŠ¡è¿æ¥é”™è¯¯: {str(e)}"
                    result["infer_service_connected"] = False
                    
            return result
            
        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": time.time()
            }


# ä¾¿æ·å‡½æ•°
def create_text_adapter(config: Optional[Dict[str, Any]] = None) -> TextModelAdapter:
    """
    åˆ›å»ºæ–‡æœ¬æ¨¡å‹é€‚é…å™¨å®ä¾‹
    
    Args:
        config: é…ç½®å‚æ•°
        
    Returns:
        TextModelAdapterå®ä¾‹
    """
    return TextModelAdapter(config)
