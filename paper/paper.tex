\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx, booktabs, tabularx}
\usepackage{xcolor, listings, placeins, microtype}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{mathptmx}
\usepackage{hyperref}
\usepackage{cleveref}
\graphicspath{{output/experiment_results/charts/}{experiment/experiment_results/charts/}}

\setlength{\columnsep}{0.6cm}
\pagestyle{empty}
\tolerance=1000
\raggedbottom
\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  citecolor=blue!60!black,
  urlcolor=blue!70!black,
  pdfauthor={Leslie Qi},
  pdftitle={xy-core: A Three-Layer Asynchronous Agent Runtime for Edge-Constrained Systems}
}

% 确保首页没有页码
\thispagestyle{empty}
\pagenumbering{gobble} % 完全禁用页码显示

% --- Code style ---
\lstdefinestyle{arxivstyle}{
  backgroundcolor=\color[gray]{0.97},
  basicstyle=\ttfamily\scriptsize,
  frame=single,
  rulecolor=\color{black!30},
  breaklines=true,
  breakatwhitespace=false,
  showstringspaces=false,
  numbers=left,
  numbersep=5pt,
  xleftmargin=0pt,
  framexleftmargin=0pt,
  keywordstyle=\bfseries\color{blue!70!black},
  commentstyle=\itshape\color{gray!60!black},
  captionpos=b,
  linewidth=\linewidth,
  xrightmargin=0pt,
  framexrightmargin=0pt
}
\lstset{style=arxivstyle,language=Python}

\title{xy-core: A Three-Layer Asynchronous Agent Runtime for Edge-Constrained Systems}
\author{Leslie Qi \\ Independent Researcher, Chongqing, China \\ \href{mailto:2991731868@qq.com}{2991731868@qq.com} \\ ORCID: 0009-0002-2954-3706}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Deploying complex AI agents on edge devices presents a fundamental conflict between the heavy computational demands of deep learning models and the latency-sensitive nature of user interactions. Traditional synchronous blocking architectures fail to maintain Quality of Service (QoS) under these constraints, while naive asynchronous implementations often suffer from resource contention and unbounded latency spikes. This paper introduces \textbf{xy-core}, an agent runtime system built upon a novel three-layer architecture: the Interaction \& State Layer, the Async Scheduling Layer, and the Worker Execution Layer. The core contribution is the Async Scheduling Layer, which implements an intelligent, non-blocking orchestration mechanism that decouples high-priority control flow from heavy computational workloads. By enforcing strict worker isolation, backpressure mechanisms, and GPU-aware load balancing, xy-core ensures main-thread responsiveness even under heavy inference loads. We evaluate the system using an "Edge Constraints Simulation" environment on RTX 5070 hardware, simulating memory caps and concurrency limits. Results demonstrate that xy-core significantly reduces main-thread blocking time compared to single-threaded and naive async baselines, maintaining high QoS and stable worker throughput in resource-constrained scenarios.
\end{abstract}

\textbf{Keywords:} Agent Runtime, Asynchronous Systems, Edge Computing, System Architecture, Scheduler Design

\section{Introduction}

The proliferation of Large Language Models (LLMs) and multimodal AI has enabled the development of sophisticated AI agents capable of complex reasoning and interaction \cite{dettmers2023qlora, lin2024awq}. However, deploying these agents in edge computing environments—characterized by limited memory, constrained CPU cores, and thermal throttles—remains a significant systems challenge \cite{li2024distributed}. Unlike cloud-based inference clusters, edge devices must handle both heavy computational tasks (e.g., LLM inference, TTS synthesis) and latency-critical interaction loops (e.g., user input handling, state updates) on shared hardware \cite{wen2025hierarchical}.

In traditional \textbf{Single-threaded Blocking Models}, a long-running inference task blocks the entire execution flow, rendering the agent unresponsive to user inputs or cancellation requests. While \textbf{Naive Async/Await Models} attempt to mitigate this via cooperative multitasking, they often fail to isolate CPU-bound operations effectively from the event loop, leading to "event loop blocking" where the control plane freezes during heavy computation. Furthermore, unmanaged concurrency in naive async systems can lead to out-of-memory (OOM) errors on devices with strict memory limits \cite{juan2023gpu}.

To address these limitations, we propose \textbf{xy-core}, a specialized agent runtime designed for edge constraints. The design philosophy centers on a strict separation of concerns through a \textbf{Three-Layer Architecture}:
\begin{enumerate}
    \item \textbf{Interaction \& State Layer}: Handles lightweight, latency-sensitive operations.
    \item \textbf{Async Scheduling Layer}: The system's core, responsible for task orchestration, resource arbitration, and backpressure.
    \item \textbf{Worker Execution Layer}: Encapsulates heavy computations in isolated execution contexts.
\end{enumerate}

Our primary contribution is the \textbf{Async Scheduling Layer}, which acts as a middleware to bridge the gap between high-frequency interaction events and low-frequency, high-latency computational jobs. By implementing QoS-aware scheduling and worker isolation, xy-core ensures that the main thread remains responsive (sub-millisecond latency) even when the underlying hardware is saturated with inference tasks.

\section{System Overview}

The architecture of xy-core is driven by the need to maintain high availability of the control plane while maximizing the throughput of the data plane (inference workers). The system is structured into three distinct vertical layers.

\subsection{Design Goals}
\begin{itemize}
    \item \textbf{Non-blocking Control Plane}: The main event loop must never be blocked by computational tasks.
    \item \textbf{Resource Isolation}: Faults or stalls in one worker (e.g., a stuck LLM inference) must not crash the system or freeze other components.
    \item \textbf{Adaptive Concurrency}: The system must dynamically adjust concurrency levels based on available system resources to prevent thrashing.
\end{itemize}

\section{Architecture Design}

This section details the implementation of the three-layer model.

\subsection{Interaction \& State Layer}
The top layer is responsible for external communication and internal state management. It interfaces with users via HTTP/WebSocket and maintains the agent's short-term memory and context. This layer is designed to be exclusively I/O-bound. All operations here are non-blocking and yield control to the event loop immediately. By restricting this layer to lightweight operations, we guarantee that the system remains responsive to interrupts (e.g., a user pressing a "Stop" button) regardless of the backend load.

\subsection{Async Scheduling Layer}
The \textbf{Async Scheduling Layer} is the architectural centerpiece of xy-core, powered by the \texttt{GlobalTaskScheduler}. It mediates between the high-speed requests from the Interaction Layer and the resource-intensive operations of the Worker Layer. This unified scheduler replaces legacy separate processors, providing a holistic view of system resources.

\subsubsection{Internal Mechanism}
The scheduler utilizes a priority-based task queue system. Incoming tasks are classified into different QoS classes:
\begin{itemize}
    \item \textbf{Real-time (RT)}: User interrupts, control signals.
    \item \textbf{Interactive (INT)}: Text generation, speech synthesis.
    \item \textbf{Background (BG)}: Context summarization, log analysis.
\end{itemize}

The scheduler employs a custom event loop policy that prioritizes RT tasks. To prevent the "thundering herd" problem common in naive async systems, we implement \textbf{Backpressure} and \textbf{Concurrency Limits}. When the worker pool is saturated, the scheduler rejects or queues new non-critical requests, propagating backpressure signals to the Interaction Layer.

\subsubsection{Main Thread QoS Protection}
A critical design invariant is that the main thread (running the asyncio event loop) performs \emph{zero} heavy computation. The scheduler monitors the "heartbeat" of the event loop. If the loop lag exceeds a threshold (e.g., 50ms), the scheduler aggressively throttles task dispatching to recover responsiveness.

\subsection{Worker Execution Layer}
The bottom layer consists of specialized workers that execute the actual heavy lifting.

\subsubsection{Worker Isolation}
To support diverse workloads (LLM, TTS, STT, Image Generation), we employ distinct isolation strategies:
\begin{itemize}
    \item \textbf{Thread-based Isolation}: For I/O-bound tasks that do not release the Python GIL (Global Interpreter Lock) immediately or rely on blocking C-extensions.
    \item \textbf{Process-based Isolation}: For CPU-intensive tasks to bypass the GIL completely.
    \item \textbf{Device-based Isolation}: For GPU tasks. We implement a \textbf{GPU Load-Aware Scheduler} that manages exclusive access to the GPU for large models, preventing VRAM fragmentation and context switching overhead.
\end{itemize}

\section{Edge Constraints Simulation}

Conducting reproducible systems research on physical edge hardware (e.g., embedded ARM boards) is often plagued by hardware variability and lack of instrumentation. To address this, we employ an \textbf{Edge Constraints Simulation} approach on a workstation equipped with an NVIDIA RTX 5070 (32GB RAM).

We simulate edge conditions through software-defined constraints:
\begin{itemize}
    \item \textbf{Memory Cap}: We use cgroups/operating system limits to restrict the available RAM to 16GB, forcing the system to handle paging and OOM risks.
    \item \textbf{Concurrency Limit}: We artificially cap the thread and process pool sizes to mimic the limited core count (e.g., 2-4 cores) of edge SoCs.
    \item \textbf{Lazy Loading}: To simulate slow storage I/O, we enforce lazy loading policies for model weights, measuring the cold-start penalties.
    \item \textbf{Resource Quotas}: Workers are assigned strict CPU/GPU time slices to simulate thermal throttling behavior.
\end{itemize}

This simulation approach allows us to collect high-fidelity telemetry (using the host's superior profiling tools) while subjecting the runtime logic to the same pressures found in physical edge deployments.

\section{Experimental Setup}

\subsection{Experimental Objectives}
The primary goal is to verify whether the Async Scheduling Layer effectively eliminates main-thread blocking and improves overall system QoS compared to baseline architectures.

\subsection{Environment & Workloads}
\begin{itemize}
    \item \textbf{Hardware}: Intel Core i7, 32GB DDR5 RAM, NVIDIA RTX 5070.
    \item \textbf{Task Injection}: We use a synthetic load generator to inject a mix of tasks:
    \begin{itemize}
        \item \textbf{LLM Inference}: Memory-bandwidth bound, high latency.
        \item \textbf{PyTorch Model Training}: Compute bound, sustained load.
        \item \textbf{TTS Synthesis}: Latency sensitive, bursty.
        \item \textbf{Blocking I/O}: Simulated network timeouts and disk writes.
    \end{itemize}
\end{itemize}

\subsection{Metrics}
\begin{itemize}
    \item \textbf{Main-thread Latency}: The time delay between an event becoming ready and the event loop executing it. This is the proxy for "responsiveness."
    \item \textbf{QoS (Quality of Service)}: Percentage of tasks completed within their deadline.
    \item \textbf{Worker Throughput}: Tasks completed per second.
    \item \textbf{Blocking Time}: Total duration the main thread was blocked by application logic.
\end{itemize}

\section{Baselines}

We compare xy-core against two common architectural patterns:

\begin{enumerate}
    \item \textbf{Single-threaded Blocking Model}: A standard synchronous implementation where tasks are executed sequentially. This represents the simplest implementation often found in scripts and prototypes.
    \item \textbf{Naive Async/Await Model}: A standard Python \texttt{asyncio} implementation without explicit worker isolation or scheduling logic. It relies on the default event loop and simple \texttt{await} calls, often falling prey to blocking libraries.
    \item \textbf{xy-core (Ours)}: The proposed three-layer architecture with the Async Scheduling Layer.
\end{enumerate}

\section{Results \& Analysis}

\subsection{Main-thread Blocking Analysis}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{main_thread_latency_en.pdf}
    \caption{Main Thread Latency Comparison. xy-core maintains near-zero blocking compared to baselines.}
    \label{fig:latency}
\end{figure}

\Cref{fig:latency} illustrates the main thread latency for the three architectures. The Single-threaded model shows linear blocking growth with task duration. The Naive Async model improves upon this but exhibits significant spikes (up to 700ms) when CPU-bound tasks inadvertently block the loop. In contrast, xy-core maintains a near-zero blocking profile (approximately 5ms), demonstrating a 99.3\% improvement in responsiveness. This validates the effectiveness of the offloading and isolation strategy.

\subsection{Throughput vs. Latency Trade-off}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{queue_wait_time_en.pdf}
    \caption{Queue Wait Time Analysis. xy-core introduces controlled waiting (backpressure) to prevent system overload.}
    \label{fig:queue}
\end{figure}

Under high concurrency (simulating up to 20 concurrent requests), the Single-threaded model's latency explodes. The Naive Async model achieves higher raw throughput but suffers from unpredictable tail latencies. xy-core maintains a stable performance by applying backpressure—rejecting excess load to preserve the responsiveness of accepted tasks. As shown in \Cref{fig:queue}, xy-core introduces controlled queue wait times (up to 17s at concurrency 20) to ensure system stability. Despite this, it achieves a 9.6\% improvement in throughput at concurrency 5 compared to the naive baseline, as it avoids context switching overhead and resource contention.

\subsection{Task Execution Distribution}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{execution_distribution_en.pdf}
    \caption{Task Execution Time Distribution in xy-core.}
    \label{fig:distribution}
\end{figure}

\Cref{fig:distribution} shows the distribution of task execution times. xy-core ensures that heavy tasks (e.g., LLM inference) do not interfere with short-lived tasks, maintaining a predictable execution profile. In the Edge Constraints Simulation (limited to 4GB RAM), the Naive Async model frequently crashes due to OOM errors as it attempts to spawn unlimited coroutines. xy-core's concurrency limiter effectively clamps memory usage below the threshold.

\section{Discussion}

\subsection{Comparison with Existing Frameworks}
While frameworks like LangChain or Celery offer task orchestration, they often lack the fine-grained control required for edge-constrained heterogeneous computing. Celery's process-based model introduces significant IPC overhead (serialization costs) which is prohibitive for high-frequency multimodal interactions. LangChain's async implementation largely delegates to the underlying library, often failing to prevent GIL contention during heavy tokenization or embedding operations. xy-core's hybrid approach---combining `asyncio` for I/O, `multiprocessing` for CPU-bound logic, and dedicated CUDA streams for GPU tasks---provides a more resource-efficient alternative specifically optimized for the resource-constrained edge profile.

\subsection{Scheduling Overhead}
The introduction of an explicit scheduling layer adds a small constant overhead (approx. 0.5ms per task) compared to raw function calls. For extremely short-lived tasks (<1ms), this overhead is measurable. However, for AI agent workloads where tasks typically take 100ms to seconds, this overhead is negligible (<1\%) and is a worthy trade-off for system stability.

\subsection{Complexity vs. Robustness}
Implementing the three-layer architecture increases code complexity. Developers must strictly adhere to the async/sync boundary. However, this strictness enforces better engineering practices, making the system easier to debug and maintain in the long run.

\section{Conclusion & Future Work}

This paper presented xy-core, a systems-oriented solution to the challenge of running AI agents on edge devices. By adopting a Three-Layer Architecture and introducing a dedicated Async Scheduling Layer, we successfully decoupled control-plane responsiveness from data-plane heaviness. Our evaluation in a simulated edge environment confirms that xy-core eliminates main-thread blocking and enforces strict resource boundaries, outperforming traditional synchronous and naive asynchronous baselines.

Future work will focus on extending the scheduler to support heterogeneous worker clusters (e.g., hybrid CPU-NPU scheduling) and implementing pre-emptive multitasking for long-running inference jobs.

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
