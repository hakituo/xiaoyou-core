\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx, booktabs, tabularx}
\usepackage{xcolor, listings, placeins, microtype}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{mathptmx}
\usepackage{hyperref}
\usepackage{cleveref}
\graphicspath{{output/experiment_results/charts/}{experiment/experiment_results/charts/}}

\setlength{\columnsep}{0.6cm}
\pagestyle{empty}
\tolerance=1000
\raggedbottom
\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  citecolor=blue!60!black,
  urlcolor=blue!70!black,
  pdfauthor={Leslie Qi},
  pdftitle={xy-core: A Three-Layer Asynchronous Agent Runtime for Edge-Constrained Systems}
}

% 确保首页没有页码
\thispagestyle{empty}
\pagenumbering{gobble} % 完全禁用页码显示

% --- Code style ---
\lstdefinestyle{arxivstyle}{
  backgroundcolor=\color[gray]{0.97},
  basicstyle=\ttfamily\scriptsize,
  frame=single,
  rulecolor=\color{black!30},
  breaklines=true,
  breakatwhitespace=false,
  showstringspaces=false,
  numbers=left,
  numbersep=5pt,
  xleftmargin=0pt,
  framexleftmargin=0pt,
  keywordstyle=\bfseries\color{blue!70!black},
  commentstyle=\itshape\color{gray!60!black},
  captionpos=b,
  linewidth=\linewidth,
  xrightmargin=0pt,
  framexrightmargin=0pt
}
\lstset{style=arxivstyle,language=Python}

\title{xy-core: A Three-Layer Asynchronous Agent Runtime for Edge-Constrained Systems}
\author{Leslie Qi \\ Independent Researcher, Chongqing, China \\ \href{mailto:2991731868@qq.com}{2991731868@qq.com} \\ ORCID: 0009-0002-2954-3706}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Deploying complex AI agents on edge devices presents a fundamental conflict between the heavy computational demands of deep learning models and the latency-sensitive nature of user interactions. Traditional synchronous blocking architectures fail to maintain Quality of Service (QoS) under these constraints, while naive asynchronous implementations often suffer from resource contention and unbounded latency spikes. This paper introduces \textbf{xy-core}, an agent runtime system built upon a novel three-layer architecture: the Interaction \& State Layer, the Async Scheduling Layer, and the Worker Execution Layer. The core contribution is the Async Scheduling Layer, which implements an intelligent, non-blocking orchestration mechanism that decouples high-priority control flow from heavy computational workloads. By enforcing strict worker isolation, backpressure mechanisms, and GPU-aware load balancing, xy-core ensures main-thread responsiveness even under heavy heterogeneous multimodal workloads (LLM, Vision, Speech, Image Generation). We evaluate the system using an "Edge Constraints Simulation" environment on an AMD Ryzen 9 8940HX with NVIDIA RTX 5070 Laptop GPU, simulating memory caps and concurrency limits. Results demonstrate that xy-core significantly reduces main-thread blocking time compared to single-threaded and naive async baselines, maintaining high QoS and stable worker throughput in resource-constrained scenarios.
\end{abstract}

\textbf{Keywords:} Agent Runtime, Asynchronous Systems, Edge Computing, System Architecture, Scheduler Design, Multimodal AI

\section{Introduction}

The proliferation of Large Language Models (LLMs) and multimodal AI has enabled the development of sophisticated AI agents capable of complex reasoning and interaction \cite{dettmers2023qlora, lin2024awq}. However, deploying these agents in edge computing environments—characterized by limited memory, constrained CPU cores, and thermal throttles—remains a significant systems challenge \cite{li2024distributed}. Unlike cloud-based inference clusters, edge devices must handle both heavy computational tasks (e.g., LLM inference, TTS synthesis) and latency-critical interaction loops (e.g., user input handling, state updates) on shared hardware \cite{wen2025hierarchical}.

A critical but often overlooked requirement for "Emotional" or "Personality-driven" agents is \textbf{latency consistency}. Jitter in response times can break the illusion of intelligence and empathy. For instance, an agent that pauses for 5 seconds to generate an image while blocking the audio response creates a disjointed user experience.

In traditional \textbf{Single-threaded Blocking Models}, a long-running inference task blocks the entire execution flow, rendering the agent unresponsive to user inputs or cancellation requests. While \textbf{Naive Async/Await Models} attempt to mitigate this via cooperative multitasking, they often fail to isolate CPU-bound operations effectively from the event loop, leading to "event loop blocking" where the control plane freezes during heavy computation. Furthermore, unmanaged concurrency in naive async systems can lead to out-of-memory (OOM) errors on devices with strict memory limits \cite{juan2023gpu}.

To address these limitations, we propose \textbf{xy-core}, a specialized agent runtime designed for edge constraints. The design philosophy centers on a strict separation of concerns through a \textbf{Three-Layer Architecture}:
\begin{enumerate}
    \item \textbf{Interaction \& State Layer}: Handles lightweight, latency-sensitive operations.
    \item \textbf{Async Scheduling Layer}: The system's core, responsible for task orchestration, resource arbitration, and backpressure.
    \item \textbf{Worker Execution Layer}: Encapsulates heavy computations in isolated execution contexts.
\end{enumerate}

Our primary contribution is the \textbf{Async Scheduling Layer}, which acts as a middleware to bridge the gap between high-frequency interaction events and low-frequency, high-latency computational jobs. By implementing QoS-aware scheduling and worker isolation, xy-core ensures that the main thread remains responsive (sub-millisecond latency) even when the underlying hardware is saturated with inference tasks.

\section{System Overview}

The architecture of xy-core is driven by the need to maintain high availability of the control plane while maximizing the throughput of the data plane (inference workers). The system is structured into three distinct vertical layers.

\subsection{Design Goals}
\begin{itemize}
    \item \textbf{Non-blocking Control Plane}: The main event loop must never be blocked by computational tasks.
    \item \textbf{Resource Isolation}: Faults or stalls in one worker (e.g., a stuck LLM inference) must not crash the system or freeze other components.
    \item \textbf{Adaptive Concurrency}: The system must dynamically adjust concurrency levels based on available system resources to prevent thrashing.
\end{itemize}

\section{Architecture Design}

This section details the implementation of the three-layer model.

\subsection{Interaction \& State Layer}
The top layer is responsible for external communication and internal state management. It interfaces with users via HTTP/WebSocket and maintains the agent's short-term memory and context. This layer is designed to be exclusively I/O-bound. All operations here are non-blocking and yield control to the event loop immediately. By restricting this layer to lightweight operations, we guarantee that the system remains responsive to interrupts (e.g., a user pressing a "Stop" button) regardless of the backend load.

\subsection{Async Scheduling Layer}
The \textbf{Async Scheduling Layer} is the architectural centerpiece of xy-core, powered by the \texttt{GlobalTaskScheduler}. It mediates between the high-speed requests from the Interaction Layer and the resource-intensive operations of the Worker Layer. This unified scheduler replaces legacy separate processors, providing a holistic view of system resources.

\subsubsection{Internal Mechanism}
The scheduler utilizes a priority-based task queue system. Incoming tasks are classified into different QoS classes:
\begin{itemize}
    \item \textbf{Real-time (RT)}: User interrupts, control signals.
    \item \textbf{Interactive (INT)}: Text generation, speech synthesis.
    \item \textbf{Background (BG)}: Context summarization, log analysis.
\end{itemize}

The scheduler employs a custom event loop policy that prioritizes RT tasks. To prevent the "thundering herd" problem common in naive async systems, we implement \textbf{Backpressure} and \textbf{Concurrency Limits}. When the worker pool is saturated, the scheduler rejects or queues new non-critical requests, propagating backpressure signals to the Interaction Layer.

\subsubsection{Main Thread QoS Protection}
A critical design invariant is that the main thread (running the asyncio event loop) performs \emph{zero} heavy computation. The scheduler monitors the "heartbeat" of the event loop. If the loop lag exceeds a threshold (e.g., 50ms), the scheduler aggressively throttles task dispatching to recover responsiveness.

\subsection{Worker Execution Layer}
The bottom layer consists of specialized workers that execute the actual heavy lifting.

\subsubsection{Worker Isolation}
To support diverse workloads (LLM, TTS, STT, Image Generation), we employ distinct isolation strategies:
\begin{itemize}
    \item \textbf{Thread-based Isolation}: Used for I/O-bound tasks that release the GIL, such as network requests or certain NumPy operations. This is the default mode for lightweight logic.
    \item \textbf{Process-based Isolation}: Used for CPU-intensive tasks like tokenization or audio preprocessing to bypass the GIL completely. This prevents "Stop-the-World" pauses in the main event loop.
    \item \textbf{Device-based Isolation}: Specifically designed for GPU-bound tasks (LLM Inference, Stable Diffusion). We implement a \textbf{GPU Load-Aware Scheduler} that acts as a semaphore for VRAM access. It manages exclusive access to the GPU for large models, preventing VRAM fragmentation and context switching overhead. For instance, the scheduler prevents Stable Diffusion and LLM from loading simultaneously if VRAM is insufficient, serializing their execution while keeping the rest of the system responsive.
\end{itemize}

\section{Edge Constraints Simulation}

Conducting reproducible systems research on physical edge hardware (e.g., embedded ARM boards) is often plagued by hardware variability and lack of instrumentation. To address this, we employ an \textbf{Edge Constraints Simulation} approach on a workstation equipped with an NVIDIA RTX 5070 (32GB RAM).

We simulate edge conditions through software-defined constraints:
\begin{itemize}
    \item \textbf{Memory Cap}: We use cgroups/operating system limits to restrict the available RAM to 16GB, forcing the system to handle paging and OOM risks.
    \item \textbf{Concurrency Limit}: We artificially cap the thread and process pool sizes to mimic the limited core count (e.g., 2-4 cores) of edge SoCs.
    \item \textbf{Lazy Loading}: To simulate slow storage I/O, we enforce lazy loading policies for model weights, measuring the cold-start penalties.
    \item \textbf{Resource Quotas}: Workers are assigned strict CPU/GPU time slices to simulate thermal throttling behavior.
\end{itemize}

This simulation approach allows us to collect high-fidelity telemetry (using the host's superior profiling tools) while subjecting the runtime logic to the same pressures found in physical edge deployments.

\section{Experimental Setup}

\subsection{Experimental Objectives}
The primary goal is to verify whether the Async Scheduling Layer effectively eliminates main-thread blocking and improves overall system QoS compared to baseline architectures.

\subsection{Environment \& Workloads}
\begin{itemize}
    \item \textbf{Hardware}: AMD Ryzen 9 8940HX, 32GB DDR5 RAM, NVIDIA RTX 5070 Laptop GPU.
    \item \textbf{Model Matrix}:
    \begin{itemize}
        \item \textbf{LLM}: Qwen2.5-7B-Instruct (Q4\_K\_M quantization) for reasoning.
        \item \textbf{Visual}: Qwen2-VL-2B for image understanding.
        \item \textbf{Image Gen}: Stable Diffusion (SD) for creative generation.
        \item \textbf{TTS}: GPT-SoVITS for speech synthesis.
    \end{itemize}
    \item \textbf{Task Injection}: We use a synthetic load generator to inject a mix of tasks mimicking a real-world "Emotional Reaction Chain":
    \begin{enumerate}
        \item \textbf{Perception}: Visual input (VL) analysis.
        \item \textbf{Cognition}: LLM reasoning to determine emotion and response.
        \item \textbf{Expression (Audio)}: TTS synthesis for immediate verbal feedback.
        \item \textbf{Expression (Visual)}: Stable Diffusion image generation for visual feedback.
    \end{enumerate}
    This chain tests the scheduler's ability to handle dependencies and prioritize latency-sensitive audio over compute-heavy image generation.
\end{itemize}

\subsection{Metrics}
\begin{itemize}
    \item \textbf{Main-thread Latency}: The time delay between an event becoming ready and the event loop executing it. This is the proxy for "responsiveness."
    \item \textbf{QoS (Quality of Service)}: Percentage of tasks completed within their deadline.
    \item \textbf{Worker Throughput}: Tasks completed per second.
    \item \textbf{Blocking Time}: Total duration the main thread was blocked by application logic.
\end{itemize}

\section{Baselines}

We compare xy-core against two common architectural patterns:

\begin{enumerate}
    \item \textbf{Single-threaded Blocking Model}: A standard synchronous implementation where tasks are executed sequentially. This represents the simplest implementation often found in scripts and prototypes.
    \item \textbf{Naive Async/Await Model}: A standard Python \texttt{asyncio} implementation without explicit worker isolation or scheduling logic. It relies on the default event loop and simple \texttt{await} calls, often falling prey to blocking libraries.
    \item \textbf{xy-core (Ours)}: The proposed three-layer architecture with the Async Scheduling Layer.
\end{enumerate}

\section{Results \& Analysis}

\subsection{Main-thread Blocking Analysis}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{main_thread_latency_en.pdf}
    \caption{Main Thread Latency Comparison. xy-core maintains near-zero blocking compared to baselines.}
    \label{fig:latency}
\end{figure}

\Cref{fig:latency} illustrates the main thread latency. In our latest experiments (Dec 7, 2025), xy-core achieved an average main thread blocking time of only \textbf{6.2ms}, with a maximum spike of 54.4ms even under heavy multimodal load. This represents a significant improvement over synchronous architectures which often exhibit blocking times exceeding 2000ms. This low latency ensures that the system remains responsive to HTTP interfaces and WebSocket heartbeats regardless of the background inference workload.

\subsection{Concurrency \& Back-pressure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{queue_wait_time_en.pdf}
    \caption{Queue Wait Time Analysis. xy-core introduces controlled waiting (backpressure) to prevent system overload.}
    \label{fig:queue}
\end{figure}

We evaluated system performance under varying concurrency levels using real workloads (Qwen2.5 + VL + SD + TTS):
\begin{itemize}
    \item \textbf{Low Load (Concurrency=1)}: The system achieves an end-to-end response time of 3.9s with a throughput of 0.25 RPS. The primary bottleneck is the Stable Diffusion generation (approx. 2.7s).
    \item \textbf{Medium Load (Concurrency=5)}: Average response time increases to 7.4s, but total system throughput rises to 0.45 RPS, demonstrating effective resource utilization.
    \item \textbf{High Load (Concurrency=10)}: Response time reaches 12.7s, while throughput stabilizes at 0.47 RPS.
\end{itemize}

Crucially, the stable throughput at high loads indicates that the \textbf{Back-pressure} mechanism is effective. When task generation exceeds GPU processing speed, the GlobalTaskScheduler queues tasks rather than crashing the system. During a 60-second stability test, the system successfully processed 32 multimodal task chains with zero errors.

\subsection{Multimodal Pipeline Performance}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{execution_distribution_en.pdf}
    \caption{Task Execution Time Distribution in xy-core.}
    \label{fig:distribution}
\end{figure}

\Cref{fig:distribution} shows the execution time distribution for a typical multimodal task chain. The P50 latency is 3.94s and P99 is 4.03s. The breakdown is as follows:
\begin{itemize}
    \item \textbf{LLM Inference}: ~200ms (CPU offload/Fast)
    \item \textbf{TTS Synthesis}: ~200ms (Network/IO bound)
    \item \textbf{Visual Analysis (VL)}: ~500ms (GPU/CPU mix)
    \item \textbf{Image Generation (SD)}: ~2700ms (GPU Bound, Dominant Factor)
\end{itemize}

This breakdown confirms that the GPU-bound Image Generation is the dominant factor in latency. xy-core's GPU lock mechanism ensures that these heavy tasks do not contend for VRAM, preventing CUDA Out of Memory errors while allowing CPU-bound tasks (like LLM inference on CPU offload) to proceed in parallel where possible.

From a User Experience (UX) perspective, this scheduling strategy allows the agent to start speaking (TTS ~400ms after LLM) well before the image is fully generated (3.9s). This "Streaming Response" capability is crucial for maintaining the illusion of a real-time conversation, masking the generation latency of heavy visual assets.

\section{Discussion}

\subsection{Comparison with Existing Frameworks}
While frameworks like LangChain or Celery offer task orchestration, they often lack the fine-grained control required for edge-constrained heterogeneous computing. Celery's process-based model introduces significant IPC overhead (serialization costs) which is prohibitive for high-frequency multimodal interactions. LangChain's async implementation largely delegates to the underlying library, often failing to prevent GIL contention during heavy tokenization or embedding operations. Web frameworks like FastAPI are excellent for I/O but provide no built-in protection against CPU-bound tasks blocking the event loop. xy-core's hybrid approach---combining `asyncio` for I/O, `multiprocessing` for CPU-bound logic, and dedicated CUDA streams for GPU tasks---provides a more resource-efficient alternative specifically optimized for the resource-constrained edge profile.

\subsection{Practical Implications for Edge AI}
The results have significant implications for the design of "Embodied AI" or "Emotional Companions" on edge devices. By guaranteeing main-thread responsiveness, xy-core enables developers to build agents that feel "alive." An agent can continue to nod (visual update), listen (audio buffering), or emote (state change) even while its "brain" (LLM) is deeply concentrating on a complex query. This separation of "Reflexive" (Interaction Layer) and "Reflective" (Worker Layer) behaviors is a step towards more biological cognitive architectures.

\subsection{Scheduling Overhead}
The introduction of an explicit scheduling layer adds a small constant overhead (approx. 0.5ms per task) compared to raw function calls. For extremely short-lived tasks (<1ms), this overhead is measurable. However, for AI agent workloads where tasks typically take 100ms to seconds, this overhead is negligible (<1\%) and is a worthy trade-off for system stability.

\subsection{Complexity vs. Robustness}
Implementing the three-layer architecture increases code complexity. Developers must strictly adhere to the async/sync boundary. However, this strictness enforces better engineering practices, making the system easier to debug and maintain in the long run.

\subsection{Security \& Isolation}
For edge deployments, system resilience is paramount. xy-core implements multi-layer isolation:
\begin{itemize}
    \item \textbf{Compute Isolation}: CPU-intensive tasks are offloaded to a \texttt{ThreadPoolExecutor}, preventing them from blocking the asyncio event loop.
    \item \textbf{Resource Isolation}: GPU tasks are guarded by \texttt{asyncio.Lock} primitives, preventing VRAM race conditions that typically lead to fatal CUDA errors.
    \item \textbf{Failure Isolation}: Each worker operates in a protected scope. A failure in a single task (e.g., a TTS network timeout) is trapped and reported without crashing the entire scheduler or affecting other concurrent tasks.
\end{itemize}

\section{Conclusion \& Future Work}

This paper presented xy-core, a systems-oriented solution to the challenge of running AI agents on edge devices. By adopting a Three-Layer Architecture and introducing a dedicated Async Scheduling Layer, we successfully decoupled control-plane responsiveness from data-plane heaviness. Our evaluation in a simulated edge environment confirms that xy-core eliminates main-thread blocking and enforces strict resource boundaries, outperforming traditional synchronous and naive asynchronous baselines.

Future work will focus on extending the scheduler to support heterogeneous worker clusters (e.g., hybrid CPU-NPU scheduling) and implementing pre-emptive multitasking for long-running inference jobs.

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
