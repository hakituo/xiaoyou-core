\begin{thebibliography}{1}

\bibitem{dettmers2023qlora}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, ``Qlora: Efficient
  finetuning of quantized llms,'' in {\em Advances in Neural Information
  Processing Systems (NeurIPS)}, vol.~36, 2023.

\bibitem{lin2024awq}
J.~Lin, J.~Tang, H.~Tang, X.~Zhang, Z.~Huang, C.~Ouyang, A.~Gholami, and
  K.~Keutzer, ``Awq: Activation-aware weight quantization for llm compression
  and acceleration,'' in {\em Proceedings of Machine Learning and Systems
  (MLSys)}, 2024.

\bibitem{li2024distributed}
H.~Li, X.~Li, Q.~Fan, Z.~Zhou, and Y.~Qin, ``Distributed dnn inference with
  fine-grained model partitioning in mobile edge computing networks,'' {\em
  IEEE Transactions on Mobile Computing}, vol.~23, no.~10, pp.~2210--2225,
  2024.

\bibitem{wen2025hierarchical}
A.~Wen, Y.~Fu, Z.~Liu, and H.~Tang, ``Hierarchical asynchronous federated
  learning algorithm for edge computing networks,'' {\em Journal of Internet
  Technology}, vol.~26, no.~5, 2025.

\bibitem{juan2023gpu}
J.~S. Juan and B.~Wong, ``Reducing the cost of gpu cold starts in serverless
  deep learning inference serving,'' in {\em 2023 IEEE International Conference
  on Pervasive Computing and Communications Workshops (PerCom Workshops)},
  pp.~489--494, 2023.

\end{thebibliography}
