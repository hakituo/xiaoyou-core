#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„LLMæ¨¡å‹äº¤äº’è„šæœ¬
ç”¨äºç›´æ¥ä¸å·²ä¸‹è½½çš„Qwen2.5-7B-Instructæ¨¡å‹è¿›è¡Œå¯¹è¯
æ”¯æŒGPUä¼˜åŒ–ã€äººè®¾ç³»ç»Ÿå’Œè¿›åº¦æ˜¾ç¤º
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import logging
import time
import gc
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("LLMè¿è¡Œå™¨")

# æ¨¡å‹è·¯å¾„é…ç½® - ä½¿ç”¨å·²ä¸‹è½½çš„æ¨¡å‹è·¯å¾„
MODEL_PATH = "d:\\AI\\xiaoyou-core\\models\\Qwen2.5-7B-Instruct\\Qwen\\Qwen2___5-7B-Instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æ£€æŸ¥CUDAå¯ç”¨æ€§å’Œè¯¦ç»†ä¿¡æ¯
if torch.cuda.is_available():
    logger.info(f"ä½¿ç”¨è®¾å¤‡: CUDA ({torch.cuda.get_device_name(0)})")
    logger.info(f"CUDAå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
else:
    logger.info(f"ä½¿ç”¨è®¾å¤‡: CPU")
    logger.warning("æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPUè¿è¡Œï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")

# å…¨å±€æ¨¡å‹å®ä¾‹
model = None
tokenizer = None

# äººè®¾é…ç½®
PERSONALITIES = {
    "default": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œä¼šç”¨è‡ªç„¶ã€å‹å¥½çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚",
    "ä¸“ä¸š": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIé¡¾é—®ï¼Œæ“…é•¿æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ä¿¡æ¯å’Œå»ºè®®ã€‚å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºã€‚",
    "æ´»æ³¼": "ä½ æ˜¯ä¸€ä¸ªæ´»æ³¼å¯çˆ±çš„AIåŠ©æ‰‹ï¼Œå–œæ¬¢ç”¨è½»æ¾æ„‰å¿«çš„æ–¹å¼ä¸ç”¨æˆ·äº¤æµã€‚å¯ä»¥é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å’Œå£è¯­åŒ–è¡¨è¾¾ã€‚",
    "å­¦æœ¯": "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯é¡¾é—®ï¼Œæ“…é•¿æ·±å…¥åˆ†æé—®é¢˜å¹¶æä¾›æœ‰æ·±åº¦çš„è§è§£ã€‚å›ç­”è¦é€»è¾‘æ¸…æ™°ï¼Œè®ºæ®å……åˆ†ã€‚",
    "åˆ›æ„": "ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰åˆ›é€ åŠ›çš„AIåŠ©æ‰‹ï¼Œå–œæ¬¢æå‡ºæ–°é¢–çš„æƒ³æ³•å’Œè§£å†³æ–¹æ¡ˆã€‚æ€ç»´å¯ä»¥æ›´åŠ å¼€æ”¾å’Œç‹¬ç‰¹ã€‚"
}
current_personality = "default"

def load_model():
    """åŠ è½½Qwen2.5-7B-Instructè¯­è¨€æ¨¡å‹"""
    global model, tokenizer
    
    try:
        logger.info(f"å¼€å§‹åŠ è½½è¯­è¨€æ¨¡å‹: Qwen2.5-7B-Instruct ä»è·¯å¾„: {MODEL_PATH}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True
        )
        logger.info("è¯­è¨€æ¨¡å‹åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        
        # åŠ è½½æ¨¡å‹
        print("æ­£åœ¨åŠ è½½æ¨¡å‹... (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)")
        
        # GPUä¼˜åŒ–é…ç½®
        model_kwargs = {
            "trust_remote_code": True,
            "use_safetensors": True
        }
        
        if DEVICE == "cuda":
            model_kwargs["device_map"] = "auto"
            model_kwargs["torch_dtype"] = torch.float16
            model_kwargs["low_cpu_mem_usage"] = True
            # å¯ç”¨Flash Attention (å¦‚æœæ”¯æŒ)
            model_kwargs["attn_implementation"] = "flash_attention_2" if torch.cuda.is_available() else "sdpa"
        else:
            model_kwargs["device_map"] = "cpu"
            model_kwargs["torch_dtype"] = torch.float32
            # CPUä¼˜åŒ–
            model_kwargs["low_cpu_mem_usage"] = True
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            **model_kwargs
        )
        
        # ä¼˜åŒ–æ¨¡å‹æ€§èƒ½
        if DEVICE == "cuda":
            model = model.eval()
            # å¯ç”¨æ¨ç†ä¼˜åŒ–
            try:
                model = torch.compile(model)  # å¯ç”¨PyTorchç¼–è¯‘ä¼˜åŒ–
                logger.info("å·²å¯ç”¨PyTorchç¼–è¯‘ä¼˜åŒ–")
            except Exception as e:
                logger.warning(f"æ— æ³•å¯ç”¨PyTorchç¼–è¯‘: {str(e)}")
        
        logger.info("è¯­è¨€æ¨¡å‹åŠ è½½å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"è¯­è¨€æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return False

def generate_response(prompt, max_new_tokens=500, temperature=0.7, history=None):
    """ç”ŸæˆLLMå“åº”
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
        history: å¯¹è¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    
    Returns:
        str: æ¨¡å‹ç”Ÿæˆçš„å“åº”
    """
    global model, tokenizer, current_personality
    
    if model is None or tokenizer is None:
        logger.warning("æ¨¡å‹æœªåŠ è½½ï¼Œæ­£åœ¨å°è¯•åŠ è½½...")
        if not load_model():
            return "âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œç¯å¢ƒé…ç½®"
    
    try:
        # æ„å»ºå¯¹è¯å†å²
        if history is None:
            history = []
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        history.append({"role": "user", "content": prompt})
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        input_text = ""
        for msg in history:
            if msg["role"] == "user":
                input_text += f"<|im_start|>user\n{msg['content']}<|im_end|>\n"
            elif msg["role"] == "assistant":
                input_text += f"<|im_start|>assistant\n{msg['content']}<|im_end|>\n"
        
        # æ·»åŠ å½“å‰äººè®¾å’Œå¼€å§‹åŠ©æ‰‹å›å¤çš„æ ‡è®°
        system_prompt = f"<|im_start|>system\n{PERSONALITIES[current_personality]}<|im_end|>\n"
        full_prompt = system_prompt + input_text + "<|im_start|>assistant\n"
        
        # æ¨¡å‹ç”Ÿæˆ
        start_time = time.time()
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(full_prompt, return_tensors="pt")
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
        
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
        if DEVICE == "cuda":
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                repetition_penalty=1.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.convert_tokens_to_ids("<|im_end|>"),
                streamer=streamer  # å¯ç”¨æµå¼è¾“å‡º
            )
        
        # è§£ç å®Œæ•´è¾“å‡º
        response = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        end_time = time.time()
        logger.info(f"ç”Ÿæˆå“åº”è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # æ·»åŠ æ¨¡å‹å›å¤åˆ°å†å²è®°å½•
        history.append({"role": "assistant", "content": response})
        
        # å¦‚æœå†å²è®°å½•å¤ªé•¿ï¼Œä¿ç•™æœ€è¿‘çš„å¯¹è¯
        if len(history) > 20:  # ä¿ç•™10è½®å¯¹è¯
            history = history[-20:]
        
        return response
    except Exception as e:
        logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}")
        return f"âŒ ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"

def set_personality(persona_name):
    """è®¾ç½®AIåŠ©æ‰‹çš„äººè®¾"""
    global current_personality
    if persona_name in PERSONALITIES:
        current_personality = persona_name
        print(f"âœ… äººè®¾å·²åˆ‡æ¢ä¸º: {persona_name}")
        print(f"å½“å‰äººè®¾æè¿°: {PERSONALITIES[persona_name]}")
        return True
    else:
        print(f"âŒ æœªçŸ¥çš„äººè®¾: {persona_name}")
        print(f"å¯ç”¨çš„äººè®¾: {', '.join(PERSONALITIES.keys())}")
        return False

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("\nğŸ“š å¯ç”¨å‘½ä»¤:")
    print("  exit/quit/é€€å‡º - é€€å‡ºç¨‹åº")
    print("  clear - æ¸…ç©ºå¯¹è¯å†å²")
    print("  restart - é‡å¯æ¨¡å‹")
    print("  help - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
    print("  personality [äººè®¾å] - åˆ‡æ¢AIåŠ©æ‰‹çš„äººè®¾")
    print("  list_personas - åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„äººè®¾")
    print("  current_persona - æ˜¾ç¤ºå½“å‰äººè®¾")
    print()

def list_personas():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„äººè®¾"""
    print("\nğŸ¤– å¯ç”¨äººè®¾åˆ—è¡¨:")
    for name, desc in PERSONALITIES.items():
        status = " âœ…" if name == current_personality else ""
        print(f"  â€¢ {name}{status}: {desc[:50]}...")
    print()

def main():
    """ä¸»å‡½æ•°ï¼Œäº¤äº’å¼å¯¹è¯"""
    print("\n========================================")
    print("      LLMæ¨¡å‹äº¤äº’å¼å¯¹è¯")
    print("========================================")
    print(f"æ¨¡å‹: Qwen2.5-7B-Instruct")
    print(f"è®¾å¤‡: {DEVICE}")
    print("æç¤º: è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
    print("      è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºç¨‹åº")
    print("      è¾“å…¥ 'personality æ´»æ³¼' åˆ‡æ¢äººè®¾")
    print("========================================")
    
    # åˆå§‹åŒ–æ—¶åŠ è½½æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    if not load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # å¯¹è¯å†å²
    history = []
    
    try:
        while True:
            # è·å–ç”¨æˆ·è¾“å…¥
            prompt = input("\nä½ : ")
            
            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            if prompt.lower() in ['exit', 'quit', 'é€€å‡º']:
                print("\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
                break
            elif prompt.lower() == 'clear':
                history = []
                print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            elif prompt.lower() == 'restart':
                print("ğŸ”„ æ­£åœ¨é‡å¯æ¨¡å‹...")
                # é‡Šæ”¾å†…å­˜
                global model, tokenizer
                model = None
                tokenizer = None
                gc.collect()
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                # é‡æ–°åŠ è½½æ¨¡å‹
                load_model()
                history = []
                print("âœ… æ¨¡å‹å·²é‡å¯")
                continue
            elif prompt.lower() == 'help':
                show_help()
                continue
            elif prompt.lower() == 'list_personas':
                list_personas()
                continue
            elif prompt.lower() == 'current_persona':
                print(f"\nå½“å‰äººè®¾: {current_personality}")
                print(f"äººè®¾æè¿°: {PERSONALITIES[current_personality]}")
                print()
                continue
            elif prompt.lower().startswith('personality '):
                parts = prompt.lower().split(' ', 1)
                if len(parts) > 1:
                    set_personality(parts[1])
                else:
                    print("âŒ è¯·æŒ‡å®šäººè®¾åç§°")
                    print(f"å¯ç”¨çš„äººè®¾: {', '.join(PERSONALITIES.keys())}")
                continue
            
            # ç”Ÿæˆå“åº”
            print("\næ¨¡å‹æ­£åœ¨ç”Ÿæˆå“åº”...")
            print("æ¨¡å‹:", end=" ")
            sys.stdout.flush()
            response = generate_response(prompt, history=history)
            
            # æ˜¾ç¤ºå®Œæˆæ ‡è®°
            print("\n" + "-"*50)
            print(f"å½“å‰äººè®¾: {current_personality} | è¾“å…¥ 'personality å¸®åŠ©' æŸ¥çœ‹äººè®¾åˆ‡æ¢å‘½ä»¤")
            print("-"*50)
    
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    finally:
        # æ¸…ç†èµ„æº
        print("æ­£åœ¨æ¸…ç†èµ„æº...")
        # åˆ é™¤å…¨å±€å¼•ç”¨ï¼Œå…è®¸åƒåœ¾å›æ”¶
        model = None
        tokenizer = None
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        print("âœ… èµ„æºå·²æ¸…ç†")

if __name__ == "__main__":
    main()