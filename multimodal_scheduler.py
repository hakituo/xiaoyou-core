#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€è°ƒåº¦è„šæœ¬ - é’ˆå¯¹8GBæ˜¾å­˜ä¼˜åŒ–
å®ç°æ–‡æœ¬ã€å›¾åƒç”Ÿæˆå’Œè¯­éŸ³çš„ååŒå·¥ä½œ
"""

import os
import sys
import torch
import gc
import asyncio
from concurrent.futures import ThreadPoolExecutor
import psutil

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®ä¿¡æ¯
class Config:
    # è®¾å¤‡é…ç½®
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    GPU_MEMORY_LIMIT = 8 * 1024 * 1024 * 1024  # 8GBæ˜¾å­˜é™åˆ¶
    
    # æ¨¡å‹è·¯å¾„
    TEXT_MODEL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "models", "Qwen2.5-7B-Instruct", "Qwen")
    VISION_MODEL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "models", "Qwen2-VL-7B-Instruct", "qwen")
    IMAGE_GEN_MODEL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "models", "FLUX.1-dev", "black-forest-labs")
    
    # é‡åŒ–å’Œä¼˜åŒ–è®¾ç½®
    USE_QUANTIZATION = True  # ä½¿ç”¨é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜
    QUANTIZATION_BITS = 4    # 4-bité‡åŒ–
    
    # å›¾åƒç”Ÿæˆé…ç½®
    IMAGE_WIDTH = 512
    IMAGE_HEIGHT = 512
    IMAGE_BATCH_SIZE = 1
    
    # è¯­éŸ³é…ç½®
    VOICE_ON_CPU = True

# æ¨¡å‹ç®¡ç†å™¨
class ModelManager:
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        self.text_model = None
        self.text_tokenizer = None
        self.vision_model = None
        self.vision_processor = None
        self.image_gen_pipeline = None
        self.voice_model = None
        
        # æ¨¡å‹çŠ¶æ€
        self.current_active_model = None  # å½“å‰æ¿€æ´»çš„æ¨¡å‹ç±»å‹
        self.models_loaded = {"text": False, "vision": False, "image_gen": False, "voice": False}
        
        # CPUçº¿ç¨‹æ± ç”¨äºè¯­éŸ³å¤„ç†
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        
        print(f"ğŸš€ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ’» è®¾å¤‡: {Config.DEVICE}")
        if Config.DEVICE == "cuda":
            print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ æ˜¾å­˜é™åˆ¶: 8GB")
    
    def _clear_memory(self, keep_model_type=None):
        """æ¸…ç†å†…å­˜å’Œæ˜¾å­˜ï¼Œä¿ç•™æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        print("ğŸ§¹ æ¸…ç†å†…å­˜å’Œæ˜¾å­˜...")
        
        # æ¸…ç†ä¸éœ€è¦ä¿ç•™çš„æ¨¡å‹
        if keep_model_type != "text":
            self.text_model = None
            self.text_tokenizer = None
            self.models_loaded["text"] = False
            
        if keep_model_type != "vision":
            self.vision_model = None
            self.vision_processor = None
            self.models_loaded["vision"] = False
            
        if keep_model_type != "image_gen":
            self.image_gen_pipeline = None
            self.models_loaded["image_gen"] = False
        
        # è¯­éŸ³æ¨¡å‹å§‹ç»ˆåœ¨CPUï¼Œä¸éœ€è¦æ¸…ç†
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if Config.DEVICE == "cuda":
            used = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            print(f"ğŸ“Š æ˜¾å­˜ä½¿ç”¨: {used:.2f}GB / {reserved:.2f}GB å·²ä¿ç•™")
    
    def _check_memory_availability(self):
        """æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ"""
        if Config.DEVICE != "cuda":
            return True
        
        available = Config.GPU_MEMORY_LIMIT - torch.cuda.memory_allocated()
        return available > 2 * 1024 * 1024 * 1024  # è‡³å°‘éœ€è¦2GBå¯ç”¨æ˜¾å­˜
    
    def load_text_model(self):
        """åŠ è½½æ–‡æœ¬æ¨¡å‹ï¼ˆæ”¯æŒé‡åŒ–å’ŒCPU offloadï¼‰"""
        if self.models_loaded["text"]:
            print("âœ… æ–‡æœ¬æ¨¡å‹å·²åŠ è½½")
            return True
        
        try:
            # ç¡®ä¿æœ‰è¶³å¤Ÿæ˜¾å­˜
            if not self._check_memory_availability():
                self._clear_memory(keep_model_type="text")
            
            print("ğŸ”„ åŠ è½½æ–‡æœ¬æ¨¡å‹...")
            
            # åŠ¨æ€å¯¼å…¥ä»¥é¿å…ä¸å¿…è¦çš„ä¾èµ–åŠ è½½
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            # åŠ è½½tokenizer
            self.text_tokenizer = AutoTokenizer.from_pretrained(
                Config.TEXT_MODEL_PATH,
                trust_remote_code=True
            )
            
            # å‡†å¤‡æ¨¡å‹é…ç½®
            model_kwargs = {
                "device_map": "auto",
                "torch_dtype": torch.float16,
                "trust_remote_code": True
            }
            
            # æ·»åŠ é‡åŒ–é…ç½®
            if Config.USE_QUANTIZATION:
                print(f"ğŸ” å¯ç”¨{Config.QUANTIZATION_BITS}-bité‡åŒ–")
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True if Config.QUANTIZATION_BITS == 4 else False,
                    load_in_8bit=True if Config.QUANTIZATION_BITS == 8 else False,
                    bnb_4bit_compute_dtype=torch.float16
                )
                model_kwargs["quantization_config"] = quantization_config
            
            # åŠ è½½æ¨¡å‹
            self.text_model = AutoModelForCausalLM.from_pretrained(
                Config.TEXT_MODEL_PATH,
                **model_kwargs
            )
            
            self.models_loaded["text"] = True
            self.current_active_model = "text"
            print("âœ… æ–‡æœ¬æ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def load_vision_model(self):
        """åŠ è½½è§†è§‰æ¨¡å‹"""
        if self.models_loaded["vision"]:
            print("âœ… è§†è§‰æ¨¡å‹å·²åŠ è½½")
            return True
        
        try:
            # ç¡®ä¿æœ‰è¶³å¤Ÿæ˜¾å­˜
            if not self._check_memory_availability():
                self._clear_memory(keep_model_type="vision")
            
            print("ğŸ”„ åŠ è½½è§†è§‰æ¨¡å‹...")
            
            # åŠ¨æ€å¯¼å…¥
            from transformers import AutoProcessor, AutoModelForVision2Seq
            
            self.vision_processor = AutoProcessor.from_pretrained(
                Config.VISION_MODEL_PATH,
                trust_remote_code=True
            )
            
            self.vision_model = AutoModelForVision2Seq.from_pretrained(
                Config.VISION_MODEL_PATH,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            self.models_loaded["vision"] = True
            self.current_active_model = "vision"
            print("âœ… è§†è§‰æ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ è§†è§‰æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def load_image_gen_model(self):
        """åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹"""
        if self.models_loaded["image_gen"]:
            print("âœ… å›¾åƒç”Ÿæˆæ¨¡å‹å·²åŠ è½½")
            return True
        
        try:
            # å›¾åƒç”Ÿæˆæ¨¡å‹éœ€è¦è¾ƒå¤šæ˜¾å­˜ï¼Œæ¸…ç†å…¶ä»–æ¨¡å‹
            self._clear_memory(keep_model_type="image_gen")
            
            print("ğŸ”„ åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹...")
            
            # åŠ¨æ€å¯¼å…¥
            from diffusers import AutoPipelineForText2Image
            
            # å‡†å¤‡é…ç½®
            pipe_kwargs = {
                "torch_dtype": torch.float16,
                "trust_remote_code": True
            }
            
            # å°è¯•å¯ç”¨xformersä¼˜åŒ–
            try:
                import xformers
                pipe_kwargs["use_xformers_memory_efficient_attention"] = True
                print("âš¡ å¯ç”¨xformersä¼˜åŒ–")
            except ImportError:
                print("â„¹ï¸ xformersä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›æœºåˆ¶")
            
            # åŠ è½½pipeline
            self.image_gen_pipeline = AutoPipelineForText2Image.from_pretrained(
                Config.IMAGE_GEN_MODEL_PATH,
                **pipe_kwargs
            )
            
            # ç§»åŠ¨åˆ°GPU
            if Config.DEVICE == "cuda":
                self.image_gen_pipeline = self.image_gen_pipeline.to(Config.DEVICE)
            
            self.models_loaded["image_gen"] = True
            self.current_active_model = "image_gen"
            print("âœ… å›¾åƒç”Ÿæˆæ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ å›¾åƒç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def load_voice_model(self):
        """åŠ è½½è¯­éŸ³æ¨¡å‹ï¼ˆå§‹ç»ˆåœ¨CPUï¼‰"""
        if self.models_loaded["voice"]:
            print("âœ… è¯­éŸ³æ¨¡å‹å·²åŠ è½½")
            return True
        
        try:
            print("ğŸ”„ åŠ è½½è¯­éŸ³æ¨¡å‹ï¼ˆCPUæ¨¡å¼ï¼‰...")
            # è¯­éŸ³æ¨¡å‹å®ç°ï¼ˆè¿™é‡Œæ˜¯å ä½ç¬¦ï¼Œæ ¹æ®å®é™…ä½¿ç”¨çš„è¯­éŸ³æ¨¡å‹ä¿®æ”¹ï¼‰
            self.voice_model = "VOICE_MODEL_PLACEHOLDER"
            self.models_loaded["voice"] = True
            print("âœ… è¯­éŸ³æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè¿è¡Œåœ¨CPUä¸Šï¼‰")
            return True
            
        except Exception as e:
            print(f"âŒ è¯­éŸ³æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    async def chat(self, prompt, max_new_tokens=300):
        """æ–‡æœ¬å¯¹è¯æ¥å£"""
        # åŠ è½½æ¨¡å‹
        if not self.load_text_model():
            return {"status": "error", "error": "æ— æ³•åŠ è½½æ–‡æœ¬æ¨¡å‹"}
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            def _generate_text():
                with torch.no_grad():
                    inputs = self.text_tokenizer(prompt, return_tensors="pt").to(Config.DEVICE)
                    outputs = self.text_model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=0.7,
                        top_p=0.9
                    )
                    response = self.text_tokenizer.decode(outputs[0], skip_special_tokens=True)
                    
                    # æå–ç”Ÿæˆçš„éƒ¨åˆ†
                    if response.startswith(prompt):
                        response = response[len(prompt):].strip()
                    
                    return response
            
            response = await asyncio.get_event_loop().run_in_executor(
                self.thread_pool, _generate_text
            )
            
            return {
                "status": "success",
                "response": response,
                "model_type": "text"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "model_type": "text"
            }
    
    async def describe_image(self, image_path, prompt="æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"):
        """å›¾åƒæè¿°æ¥å£"""
        # åŠ è½½æ¨¡å‹
        if not self.load_vision_model():
            return {"status": "error", "error": "æ— æ³•åŠ è½½è§†è§‰æ¨¡å‹"}
        
        try:
            from PIL import Image
            
            def _describe_image():
                with torch.no_grad():
                    # åŠ è½½å›¾åƒ
                    image = Image.open(image_path).convert("RGB")
                    
                    # å¤„ç†è¾“å…¥
                    inputs = self.vision_processor(
                        text=prompt,
                        images=image,
                        return_tensors="pt"
                    ).to(Config.DEVICE)
                    
                    # ç”Ÿæˆæè¿°
                    outputs = self.vision_model.generate(
                        **inputs,
                        max_new_tokens=512,
                        temperature=0.7
                    )
                    
                    # è§£ç è¾“å‡º
                    description = self.vision_processor.decode(
                        outputs[0], 
                        skip_special_tokens=True
                    )
                    
                    return description
            
            description = await asyncio.get_event_loop().run_in_executor(
                self.thread_pool, _describe_image
            )
            
            return {
                "status": "success",
                "description": description,
                "model_type": "vision"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "model_type": "vision"
            }
    
    async def generate_image(self, prompt, save_path="output.png", width=None, height=None):
        """å›¾åƒç”Ÿæˆæ¥å£"""
        # è®¾ç½®é»˜è®¤åˆ†è¾¨ç‡
        width = width or Config.IMAGE_WIDTH
        height = height or Config.IMAGE_HEIGHT
        
        # åŠ è½½æ¨¡å‹
        if not self.load_image_gen_model():
            return {"status": "error", "error": "æ— æ³•åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹"}
        
        try:
            def _generate_image():
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
                
                with torch.no_grad():
                    # ç”Ÿæˆå›¾åƒ
                    image = self.image_gen_pipeline(
                        prompt=prompt,
                        width=width,
                        height=height,
                        guidance_scale=0.0,
                        num_inference_steps=4
                    ).images[0]
                    
                    # ä¿å­˜å›¾åƒ
                    image.save(save_path)
                    return save_path
            
            image_path = await asyncio.get_event_loop().run_in_executor(
                self.thread_pool, _generate_image
            )
            
            return {
                "status": "success",
                "image_path": image_path,
                "model_type": "image_gen"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "model_type": "image_gen"
            }
    
    async def process_voice(self, audio_path, text=None):
        """è¯­éŸ³å¤„ç†æ¥å£ï¼ˆè¿è¡Œåœ¨CPUä¸Šï¼‰"""
        # åŠ è½½æ¨¡å‹
        if not self.load_voice_model():
            return {"status": "error", "error": "æ— æ³•åŠ è½½è¯­éŸ³æ¨¡å‹"}
        
        try:
            # è¿™é‡Œæ˜¯è¯­éŸ³å¤„ç†çš„å ä½å®ç°
            # æ ¹æ®å®é™…ä½¿ç”¨çš„è¯­éŸ³æ¨¡å‹ï¼ˆå¦‚RVCã€so-vitsç­‰ï¼‰ä¿®æ”¹
            def _process_voice():
                # æ¨¡æ‹Ÿè¯­éŸ³å¤„ç†
                # å®é™…å®ç°åº”è°ƒç”¨ç›¸åº”çš„è¯­éŸ³å¤„ç†åº“
                return "è¯­éŸ³å¤„ç†ç»“æœ"
            
            result = await asyncio.get_event_loop().run_in_executor(
                self.thread_pool, _process_voice
            )
            
            return {
                "status": "success",
                "result": result,
                "model_type": "voice"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "model_type": "voice"
            }
    
    async def process_request(self, request_data):
        """ç»Ÿä¸€å¤„ç†è¯·æ±‚æ¥å£"""
        mode = request_data.get("mode", "chat")
        
        if mode == "chat":
            return await self.chat(request_data.get("prompt", ""))
        elif mode == "describe_image":
            return await self.describe_image(
                request_data.get("image_path", ""),
                request_data.get("prompt", "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹")
            )
        elif mode == "generate_image":
            return await self.generate_image(
                request_data.get("prompt", ""),
                request_data.get("save_path", "output.png"),
                request_data.get("width", None),
                request_data.get("height", None)
            )
        elif mode == "process_voice":
            return await self.process_voice(
                request_data.get("audio_path", ""),
                request_data.get("text", None)
            )
        else:
            return {
                "status": "error",
                "error": f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}"
            }

# èµ„æºç›‘æ§
class ResourceMonitor:
    @staticmethod
    def get_system_status():
        """è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€"""
        status = {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "ram_used": psutil.virtual_memory().used / (1024 ** 3),
            "ram_total": psutil.virtual_memory().total / (1024 ** 3)
        }
        
        if torch.cuda.is_available():
            status["gpu_memory_used"] = torch.cuda.memory_allocated() / (1024 ** 3)
            status["gpu_memory_total"] = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        
        return status

# ä¸»ç¨‹åºç¤ºä¾‹
async def main():
    print("""ğŸ¯ å¤šæ¨¡æ€è°ƒåº¦ç³»ç»Ÿ - 8GBæ˜¾å­˜ä¼˜åŒ–ç‰ˆ
ğŸ”§ æ ¸å¿ƒç‰¹æ€§ï¼š
  â€¢ 4-bité‡åŒ–æ–‡æœ¬æ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨é™è‡³5-6GB
  â€¢ å›¾åƒç”Ÿæˆbatch=1ï¼Œä½åˆ†è¾¨ç‡ä¼˜åŒ–
  â€¢ è¯­éŸ³æ¨¡å‹è¿è¡Œåœ¨CPUä¸Š
  â€¢ åŠ¨æ€æ˜¾å­˜ç®¡ç†å’Œæ¨¡å‹åˆ‡æ¢
  â€¢ å¼‚æ­¥å¤„ç†æ”¯æŒ
""")
    
    # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
    manager = ModelManager()
    
    # æ‰“å°ç³»ç»ŸçŠ¶æ€
    print("ğŸ“Š ç³»ç»Ÿèµ„æºçŠ¶æ€:")
    status = ResourceMonitor.get_system_status()
    print(f"  â€¢ CPUä½¿ç”¨ç‡: {status['cpu_percent']}%")
    print(f"  â€¢ å†…å­˜ä½¿ç”¨: {status['ram_used']:.2f}GB / {status['ram_total']:.2f}GB")
    if "gpu_memory_used" in status:
        print(f"  â€¢ GPUæ˜¾å­˜: {status['gpu_memory_used']:.2f}GB / {status['gpu_memory_total']:.2f}GB")
    
    print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("  1. æ–‡æœ¬å¯¹è¯: await manager.chat('ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±')")
    print("  2. å›¾åƒæè¿°: await manager.describe_image('image.jpg')")
    print("  3. å›¾åƒç”Ÿæˆ: await manager.generate_image('ä¸€åªå¯çˆ±çš„å°çŒ«')")
    print("  4. è¯­éŸ³å¤„ç†: await manager.process_voice('audio.wav')")
    
    print("\nâœ¨ ç³»ç»Ÿå·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨å¤šæ¨¡æ€åŠŸèƒ½")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()